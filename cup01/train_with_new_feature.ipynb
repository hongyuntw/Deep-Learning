{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from summa import keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf59f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "11847\n",
      "39490\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_feature.csv')\n",
    "df_test = pd.read_csv('./test_feature.csv')\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_weekend'] = df_train['weekday'].apply(lambda x: 1 if x ==' Sat' or x == ' Sun' else 0)\n",
    "df_test['is_weekend'] = df_test['weekday'].apply(lambda x: 1 if x ==' Sat' or x == ' Sun' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1be0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # month\n",
    "# def pop_month(x):\n",
    "#     if x == 3:\n",
    "#         return 1\n",
    "#     elif x == 10:\n",
    "#         return -1 # -1 means not popular\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_month'] = df_train['month'].apply(pop_month)\n",
    "# df_test['popular_month'] = df_test['month'].apply(pop_month)\n",
    "# df_train.corr()['popular_month']['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733e9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hours\n",
    "# def pop_hour(x):\n",
    "#     if x == 13 or x == 21:\n",
    "#         return -1\n",
    "#     elif x == 5:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_hour'] = df_train['hour'].apply(pop_hour)\n",
    "# df_test['popular_hour'] = df_test['hour'].apply(pop_hour)\n",
    "# df_train.corr()['popular_hour']['Popularity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc6c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # channel \n",
    "# # just watch the EDA and assign weights\n",
    "# def pop_channel(x):\n",
    "#     if x == 'social-media' or x == 'tech':\n",
    "#         return 2\n",
    "#     elif x == 'marketing' or x == 'lifestyle':\n",
    "#         return 1\n",
    "#     elif x == 'world' or x == 'entertainment':\n",
    "#         return -2\n",
    "#     elif x == 'business':\n",
    "#         return -1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_channel'] = df_train['channel'].apply(pop_channel)\n",
    "# df_test['popular_channel'] = df_test['hour'].apply(pop_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8151617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_popularity_author(col):\n",
    "#     df = df_train.groupby(f'{col}').mean().reset_index().sort_values(by='Popularity', ascending=False) \\\n",
    "#               [[f'{col}', 'Popularity']]\n",
    "#     df.columns=[f'{col}', 'avg_popularity']\n",
    "    \n",
    "#     '''\n",
    "#     pop_5 = df[df['avg_popularity'] >= 0.7][f'{col}'].values\n",
    "#     pop_4 = df[(df['avg_popularity'] < 0.7) & (df['avg_popularity'] >= 0.6)][f'{col}'].values\n",
    "#     pop_3 = df[(df['avg_popularity'] < 0.6) & (df['avg_popularity'] >= 0.5)][f'{col}'].values\n",
    "#     pop_2 = df[(df['avg_popularity'] < 0.5) & (df['avg_popularity'] >= 0.4)][f'{col}'].values\n",
    "#     pop_1 = df[(df['avg_popularity'] < 0.4) & (df['avg_popularity'] >= 0.3)][f'{col}'].values\n",
    "#     pop_0 = df[df['avg_popularity'] < 0.3][f'{col}'].values\n",
    "#     '''\n",
    "#     pop_5 = df[df['avg_popularity'] >= 0.5][f'{col}'].values\n",
    "#     pop_2 = df[(df['avg_popularity'] >= 0.2) & (df['avg_popularity'] < 0.5)][f'{col}'].values\n",
    "#     unpop_2 = df[(df['avg_popularity'] <= -0.2) & (df['avg_popularity'] >= -0.5)][f'{col}'].values\n",
    "#     unpop_5 = df[df['avg_popularity'] < -0.5][f'{col}'].values\n",
    "    \n",
    "#     def lambda_fxn(x):\n",
    "#         '''\n",
    "#         if x in pop_5:\n",
    "#             return 5\n",
    "#         elif x in pop_4:\n",
    "#             return 4\n",
    "#         elif x in pop_3:\n",
    "#             return 3\n",
    "#         elif x in pop_2:\n",
    "#             return 2\n",
    "#         elif x in pop_1:\n",
    "#             return 1\n",
    "#         elif x in pop_0:\n",
    "#             return -1\n",
    "            \n",
    "#         # To catch news desks/sections/subsections/material in test but not in train\n",
    "#         else:\n",
    "#             return 0\n",
    "#         '''\n",
    "        \n",
    "#         if x in pop_5:\n",
    "#             return 5\n",
    "#         elif x in pop_2:\n",
    "#             return 2\n",
    "#         elif x in unpop_5:\n",
    "#             return -5\n",
    "#         elif x in unpop_2:\n",
    "#             return -2\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "    \n",
    "#     df_train[f'popular_{col}'] = df_train[f'{col}'].apply(lambda_fxn)\n",
    "#     df_test[f'popular_{col}'] = df_test[f'{col}'].apply(lambda_fxn)\n",
    "# map_popularity_author('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b85884",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'author'\n",
    "df = df_train.groupby(f'{col}').mean().reset_index().sort_values(by='Popularity', ascending=False) \\\n",
    "              [[f'{col}', 'Popularity']]\n",
    "df.columns=[f'{col}', 'avg_popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c20c3529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_avg_score = {}\n",
    "for i, row in df.iterrows():\n",
    "    author_name = row['author']\n",
    "    score = row['avg_popularity']\n",
    "    author_avg_score[author_name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5117a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['author_popularity'] = df_train['author'].apply(lambda x: author_avg_score[x] if x in author_avg_score else 0.0)\n",
    "df_test['author_popularity'] = df_test['author'].apply(lambda x: author_avg_score[x] if x in author_avg_score else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6760a8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# define extra stopwords\n",
    "extra_stopwords = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
    "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
    "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
    "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
    "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
    "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
    "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
    "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
    "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
    "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
    "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
    "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
    "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
    "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
    "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
    "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
    "                    \"you've\", \"'s\", \"'d\", \"'m\", \"abov\", \"afterward\", \"ai\", \"alon\", \"alreadi\", \"alway\", \"ani\", \n",
    "                     \"anoth\", \"anyon\", \"anyth\", \"anywher\", \"becam\", \"becaus\", \"becom\", \"befor\", \n",
    "                     \"besid\", \"ca\", \"cri\", \"dare\", \"describ\", \"did\", \"doe\", \"dure\", \"els\", \n",
    "                     \"elsewher\", \"empti\", \"everi\", \"everyon\", \"everyth\", \"everywher\", \"fifti\", \n",
    "                     \"forti\", \"gon\", \"got\", \"henc\", \"hereaft\", \"herebi\", \"howev\", \"hundr\", \"inde\", \n",
    "                     \"let\", \"ll\", \"mani\", \"meanwhil\", \"moreov\", \"n't\", \"na\", \"need\", \"nobodi\", \"noon\", \n",
    "                     \"noth\", \"nowher\", \"ol\", \"onc\", \"onli\", \"otherwis\", \"ought\", \"ourselv\", \"perhap\", \n",
    "                     \"pleas\", \"sever\", \"sha\", \"sinc\", \"sincer\", \"sixti\", \"somebodi\", \"someon\", \"someth\", \n",
    "                     \"sometim\", \"somewher\", \"ta\", \"themselv\", \"thenc\", \"thereaft\", \"therebi\", \"therefor\", \n",
    "                     \"togeth\", \"twelv\", \"twenti\", \"ve\", \"veri\", \"whatev\", \"whenc\", \"whenev\", \n",
    "                    \"wherea\", \"whereaft\", \"wherebi\", \"wherev\", \"whi\", \"wo\", \"anywh\", \"el\", \"elsewh\", \"everywh\", \n",
    "                    \"ind\", \"otherwi\", \"plea\", \"somewh\", \"yourselv\"]\n",
    "\n",
    "stop = stop + extra_stopwords\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wnl = WordNetLemmatizer()\n",
    "def prep(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [porter.stem(w) for w in tokens if w not in stop and w.isalpha()]\n",
    "    return tokens\n",
    "#     return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "#             if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae27e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# df_train['keywords'] = df_train['content'].progress_apply(prep) # 此步驟約要花五分鐘\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(tokenize)\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(lambda x: ' '.join(x))\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d9c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('./train_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d885f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "# df_test['keywords'] = df_test['content'].progress_apply(prep) # 此步驟約要花五分鐘\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(tokenize)\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(lambda x: ' '.join(x))\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb0c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('./test_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4059b93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'global_sentiment_polarity', 'global_subjectivity',\n",
       "       'title_subjectivity', 'title_sentiment_polarity',\n",
       "       'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'day_of_month', 'month', 'hour', 'is_weekend',\n",
       "       'keywords', 'author_popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2006f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topic = df_train['topic'].values.astype('U').tolist() + df_test['topic'].values.astype('U').tolist()\n",
    "all_channel = df_train['channel'].values.astype('U').tolist() + df_test['channel'].values.astype('U').tolist()\n",
    "all_titles = df_train['title'].values.astype('U').tolist() + df_test['title'].values.astype('U').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b458f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents = list(df_train['content'].values) \n",
    "test_contents = list(df_test['content'].values)\n",
    "all_contents = train_contents + test_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617e7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  \n",
    "#                                   tokenizer=tokenize,\n",
    "#                                   max_features = 5000,\n",
    "#                                   dtype = np.float32)\n",
    "\n",
    "# train_keywords_tfidf = tfidf_vectorize.fit_transform(df_train['keywords'].values.astype('U').tolist())\n",
    "# test_keywords_tfidf = tfidf_vectorize.transform(df_test['keywords'].values.astype('U').tolist())\n",
    "# print(train_keywords_tfidf.shape)\n",
    "# print(test_keywords_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239e590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  \n",
    "#                                   tokenizer=tokenize,\n",
    "#                                   max_features = 1024,\n",
    "#                                   dtype = np.float32)\n",
    "\n",
    "# train_topic_tfidf = tfidf_vectorize.fit_transform(df_train['topic'].values.astype('U').tolist())\n",
    "# test_topic_tfidf = tfidf_vectorize.transform(df_test['topic'].values.astype('U').tolist())\n",
    "# print(train_topic_tfidf.shape)\n",
    "# print(test_topic_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e507f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab  = tfidf_vectorize.get_feature_names()\n",
    "# print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8711004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  \n",
    "#                                   tokenizer=tokenize,\n",
    "#                                   ngram_range=(1,1), \n",
    "#                                   sublinear_tf = True,\n",
    "# #                                   max_features = 2048,\n",
    "#                                   dtype = np.float32)\n",
    "# all_content_tfidf = tfidf_vectorize.fit_transform(all_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecbebcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_content_tfidf = tfidf_vectorize.transform(train_contents)\n",
    "# test_content_tfidf = tfidf_vectorize.transform(test_contents)\n",
    "# print(train_content_tfidf.shape)\n",
    "# print(test_content_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be097792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD, PCA\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# svd.fit(all_content_tfidf)\n",
    "\n",
    "# train_content_svd = svd.transform(train_content_tfidf)\n",
    "# test_content_svd = svd.transform(test_content_tfidf)\n",
    "\n",
    "# print(train_content_svd.shape)\n",
    "# print(test_content_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0ab332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_topic_tfidf = tfidf_vectorize.transform(df_train['topic'].values.astype('U').tolist())\n",
    "# test_topic_tfidf = tfidf_vectorize.transform(df_test['topic'].values.astype('U').tolist())\n",
    "\n",
    "# print(train_topic_tfidf.shape)\n",
    "# print(test_topic_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f51f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_title_tfidf = tfidf_vectorize.transform(df_train['title'].values.astype('U').tolist())\n",
    "# test_title_tfidf = tfidf_vectorize.transform(df_test['title'].values.astype('U').tolist())\n",
    "\n",
    "# print(train_title_tfidf.shape)\n",
    "# print(test_title_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea828093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/harry/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 1024)\n",
      "(11847, 1024)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=1024,\n",
    "                            preprocessor=prep,\n",
    "                            tokenizer=tokenize,\n",
    "                           dtype = np.float32)\n",
    "\n",
    "train_topic_hash = hashvec.fit_transform(df_train['topic'].values.astype('U').tolist())\n",
    "test_topic_hash = hashvec.transform(df_test['topic'].values.astype('U').tolist())\n",
    "print(train_topic_hash.shape)\n",
    "print(test_topic_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc425f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hashvec = HashingVectorizer(n_features=1024,\n",
    "#                            dtype = np.float32)\n",
    "\n",
    "# train_keywords_hash = hashvec.fit_transform(df_train['keywords'].values.astype('U').tolist())\n",
    "# test_keywords_hash = hashvec.transform(df_test['keywords'].values.astype('U').tolist())\n",
    "# print(train_keywords_hash.shape)\n",
    "# print(test_keywords_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5adb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_content_hash = hashvec.fit_transform(train_contents)\n",
    "# test_content_hash = hashvec.transform(test_contents)\n",
    "# print(train_content_hash.shape)\n",
    "# print(test_content_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "012a14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_title_hash = hashvec.transform(df_train['title'].values.astype('U').tolist())\n",
    "# test_title_hash = hashvec.transform(df_test['title'].values.astype('U').tolist())\n",
    "# print(train_title_hash.shape)\n",
    "# print(test_title_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93edc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f740898c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 33)\n",
      "(11847, 33)\n"
     ]
    }
   ],
   "source": [
    "# ohe channel\n",
    "OHE_channel = OneHotEncoder(handle_unknown='ignore')\n",
    "train_ohe_channel = OHE_channel.fit_transform(df_train['channel'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_channel = OHE_channel.transform(df_test['channel'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_channel.shape)\n",
    "print(test_ohe_channel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93a465c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 7)\n",
      "(11847, 7)\n"
     ]
    }
   ],
   "source": [
    "# ohe weekday\n",
    "OHE_weekday = OneHotEncoder(handle_unknown='ignore')\n",
    "all_week_day = list(df_train['weekday'].values) +  list(df_test['weekday'].values)\n",
    "# OHE_weekday.fit(np.array(all_week_day).reshape(-1,1))\n",
    "\n",
    "train_ohe_weekday = OHE_weekday.fit_transform(df_train['weekday'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_weekday = OHE_weekday.transform(df_test['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_weekday.shape)\n",
    "print(test_ohe_weekday.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "055376c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 428)\n",
      "(11847, 428)\n"
     ]
    }
   ],
   "source": [
    "# ohe author\n",
    "OHE_author = OneHotEncoder(handle_unknown='ignore')\n",
    "all_author = list(df_train['author'].values) +  list(df_test['author'].values)\n",
    "# OHE_author.fit(np.array(all_author).reshape(-1,1))\n",
    "\n",
    "train_ohe_author = OHE_author.fit_transform(df_train['author'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_author = OHE_author.transform(df_test['author'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "print(train_ohe_author.shape)\n",
    "print(test_ohe_author.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9df222d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the numerical features\n",
    "# from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# # scaler = StandardScaler()\n",
    "# need_to_normalize = [\n",
    "#     'img count', \n",
    "#     'media count',\n",
    "#     'n_tokens_content',\n",
    "#     'n_tokens_title', \n",
    "#     'num_hrefs', \n",
    "#     'num_self_hrefs',\n",
    "# #     'day_of_month',\n",
    "# #     'month',\n",
    "# #     'hour',\n",
    "# #     'rate_positive_words',\n",
    "# #     'rate_negative_words',\n",
    "# #     'avg_positive_polarity',\n",
    "# #     'avg_negative_polarity',\n",
    "# #     'max_positive_polarity',\n",
    "# #     'min_positive_polarity',\n",
    "# #     'max_negative_polarity',\n",
    "# #     'min_negative_polarity',\n",
    "# #     'abs_title_subjectivity',\n",
    "# #     'abs_title_sentiment_polarity',\n",
    "# #     'popular_month',\n",
    "# #     'popular_hour',\n",
    "# #     'popular_channel',\n",
    "# #     'popular_author',\n",
    "#     ]\n",
    "\n",
    "# df_train_normalize = df_train.copy(deep=True)\n",
    "# df_test_normalize = df_test.copy(deep=True)\n",
    "\n",
    "# # scaler.fit(df_all[numerical])\n",
    "# df_train_normalize[need_to_normalize] = scaler.fit_transform(df_train[need_to_normalize])\n",
    "# df_test_normalize[need_to_normalize] = scaler.transform(df_test[need_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6566f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'global_sentiment_polarity', 'global_subjectivity',\n",
       "       'title_subjectivity', 'title_sentiment_polarity',\n",
       "       'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'day_of_month', 'month', 'hour', 'is_weekend',\n",
       "       'keywords', 'author_popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "869cf72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 38)\n",
      "(27643, 69)\n"
     ]
    }
   ],
   "source": [
    "df_train_concat = pd.concat([df_train[['Popularity',\n",
    "                                                'img count', \n",
    "                                                'media count',\n",
    "                                                'n_tokens_title',\n",
    "                                                'n_tokens_content',\n",
    "                                                'n_unique_tokens',\n",
    "                                                'n_non_stop_words',\n",
    "                                                'n_non_stop_unique_tokens',\n",
    "                                                'num_hrefs',\n",
    "                                                'num_self_hrefs',\n",
    "                                                'global_sentiment_polarity',\n",
    "                                                'global_subjectivity',\n",
    "                                                'title_subjectivity',\n",
    "                                                'title_sentiment_polarity',\n",
    "                                                'abs_title_subjectivity',\n",
    "                                                'abs_title_sentiment_polarity',\n",
    "                                                'rate_positive_words',\n",
    "                                                'rate_negative_words',\n",
    "                                                'avg_positive_polarity',\n",
    "                                                'min_positive_polarity',\n",
    "                                                'max_positive_polarity',\n",
    "                                                'avg_negative_polarity',\n",
    "                                                'min_negative_polarity',\n",
    "                                                'max_negative_polarity',\n",
    "                                                'day_of_month',\n",
    "                                                'month',\n",
    "                                                'hour',\n",
    "                                                'is_weekend',\n",
    "#                                                 'popular_month',\n",
    "#                                                 'popular_hour',\n",
    "#                                                 'popular_channel',\n",
    "#                                                 'popular_author',\n",
    "                                                'author_popularity',\n",
    "\n",
    "                                               ]], \n",
    "#                       pd.DataFrame(train_topic_hash.toarray()),\n",
    "                      pd.DataFrame(train_ohe_channel, columns=OHE_channel.get_feature_names()),\n",
    "                      pd.DataFrame(train_ohe_weekday, columns=OHE_weekday.get_feature_names()),\n",
    "#                       pd.DataFrame(train_ohe_author,  columns=OHE_author.get_feature_names()),\n",
    "                     ], axis=1)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37700187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 37)\n",
      "(11847, 68)\n"
     ]
    }
   ],
   "source": [
    "df_test_concat = pd.concat([df_test[[\n",
    "                                                'img count', \n",
    "                                                'media count',\n",
    "                                                'n_tokens_title',\n",
    "                                                'n_tokens_content',\n",
    "                                                'n_unique_tokens',\n",
    "                                                'n_non_stop_words',\n",
    "                                                'n_non_stop_unique_tokens',\n",
    "                                                'num_hrefs',\n",
    "                                                'num_self_hrefs',\n",
    "                                                'global_sentiment_polarity',\n",
    "                                                'global_subjectivity',\n",
    "                                                'title_subjectivity',\n",
    "                                                'title_sentiment_polarity',\n",
    "                                                'abs_title_subjectivity',\n",
    "                                                'abs_title_sentiment_polarity',\n",
    "                                                'rate_positive_words',\n",
    "                                                'rate_negative_words',\n",
    "                                                'avg_positive_polarity',\n",
    "                                                'min_positive_polarity',\n",
    "                                                'max_positive_polarity',\n",
    "                                                'avg_negative_polarity',\n",
    "                                                'min_negative_polarity',\n",
    "                                                'max_negative_polarity',\n",
    "                                                'day_of_month',\n",
    "                                                'month',\n",
    "                                                'hour',\n",
    "                                                'is_weekend',\n",
    "#                                                 'popular_month',\n",
    "#                                                 'popular_hour',\n",
    "#                                                 'popular_channel',\n",
    "#                                                 'popular_author',                  \n",
    "                                                 'author_popularity',\n",
    "\n",
    "                                               ]], \n",
    "#                       pd.DataFrame(test_topic_hash.toarray()),\n",
    "                      pd.DataFrame(test_ohe_channel, columns=OHE_channel.get_feature_names()),\n",
    "                      pd.DataFrame(test_ohe_weekday, columns=OHE_weekday.get_feature_names()),\n",
    "#                       pd.DataFrame(test_ohe_author,  columns=OHE_author.get_feature_names()),\n",
    "                     ], axis=1)\n",
    "\n",
    "print(df_test.shape)\n",
    "print(df_test_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28424861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>img count</th>\n",
       "      <th>media count</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_viral</th>\n",
       "      <th>x0_watercooler</th>\n",
       "      <th>x0_world</th>\n",
       "      <th>x0_ Fri</th>\n",
       "      <th>x0_ Mon</th>\n",
       "      <th>x0_ Sat</th>\n",
       "      <th>x0_ Sun</th>\n",
       "      <th>x0_ Thu</th>\n",
       "      <th>x0_ Tue</th>\n",
       "      <th>x0_ Wed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>575</td>\n",
       "      <td>0.507826</td>\n",
       "      <td>0.615652</td>\n",
       "      <td>0.424348</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>305</td>\n",
       "      <td>0.478689</td>\n",
       "      <td>0.619672</td>\n",
       "      <td>0.357377</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.488829</td>\n",
       "      <td>0.638963</td>\n",
       "      <td>0.421805</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>274</td>\n",
       "      <td>0.711679</td>\n",
       "      <td>0.733577</td>\n",
       "      <td>0.583942</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1370</td>\n",
       "      <td>0.497080</td>\n",
       "      <td>0.756934</td>\n",
       "      <td>0.464964</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Popularity  img count  media count  n_tokens_title  n_tokens_content  \\\n",
       "0           0          1            0               8               575   \n",
       "1           1          2            0              12               305   \n",
       "2           1          2           25              12              1119   \n",
       "3           0          1           21               5               274   \n",
       "4           0         52            1              10              1370   \n",
       "\n",
       "   n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  \\\n",
       "0         0.507826          0.615652                  0.424348         22   \n",
       "1         0.478689          0.619672                  0.357377         18   \n",
       "2         0.488829          0.638963                  0.421805         11   \n",
       "3         0.711679          0.733577                  0.583942         13   \n",
       "4         0.497080          0.756934                  0.464964         16   \n",
       "\n",
       "   num_self_hrefs  ...  x0_viral  x0_watercooler  x0_world  x0_ Fri  x0_ Mon  \\\n",
       "0               0  ...       0.0             0.0       1.0      0.0      0.0   \n",
       "1               3  ...       0.0             0.0       0.0      0.0      0.0   \n",
       "2               4  ...       0.0             0.0       0.0      0.0      0.0   \n",
       "3               6  ...       0.0             1.0       0.0      1.0      0.0   \n",
       "4               7  ...       0.0             0.0       0.0      0.0      0.0   \n",
       "\n",
       "   x0_ Sat  x0_ Sun  x0_ Thu  x0_ Tue  x0_ Wed  \n",
       "0      0.0      0.0      0.0      0.0      1.0  \n",
       "1      0.0      0.0      1.0      0.0      0.0  \n",
       "2      0.0      0.0      0.0      0.0      1.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0  \n",
       "4      0.0      0.0      1.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04f5d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = abs(df_train_concat._get_numeric_data().corr()['Popularity']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6c736b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# keep_cols = []\n",
    "# th = 0.004\n",
    "# for index, value in feats.items():\n",
    "# #     print(f\"Index : {index} \\t\\t, Value : {value}\")\n",
    "#     if value > th and index != 'Popularity':\n",
    "#         keep_cols.append(index)\n",
    "# print(len(keep_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc290919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_cols = list(feats.keys())\n",
    "# keep_cols = keep_cols[:1001]\n",
    "# keep_cols.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be78fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(keep_cols[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91e22ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 68) (27643,)\n",
      "(11847, 68)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train_concat.drop(['Popularity'], axis=1).to_numpy()\n",
    "x_test = df_test_concat.to_numpy()\n",
    "y_train = df_train['Popularity'].to_numpy()\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81d24483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "# kfold = StratifiedKFold(n_splits = 5, random_state = 2021 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "963869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "# d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a4da910",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.05, \n",
    "              'max_depth': 4, \n",
    "              'subsample': 0.7 ,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'min_child_weight' : 4,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc', \n",
    "#               'lambda': 1.5,\n",
    "              'alpha': 0.005,\n",
    "#               'n_estimators': 119,\n",
    "             }\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_params = xgb_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42fb5855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.56057+0.00340\ttest-auc:0.54795+0.00360\n",
      "[10]\ttrain-auc:0.61147+0.00341\ttest-auc:0.59486+0.00629\n",
      "[20]\ttrain-auc:0.61805+0.00436\ttest-auc:0.59756+0.00873\n",
      "[30]\ttrain-auc:0.62314+0.00212\ttest-auc:0.59886+0.00832\n",
      "[40]\ttrain-auc:0.62713+0.00204\ttest-auc:0.59974+0.00880\n",
      "[50]\ttrain-auc:0.63181+0.00160\ttest-auc:0.60025+0.00924\n",
      "[60]\ttrain-auc:0.63734+0.00168\ttest-auc:0.60058+0.00954\n",
      "[70]\ttrain-auc:0.64261+0.00189\ttest-auc:0.60055+0.00923\n",
      "[80]\ttrain-auc:0.64763+0.00171\ttest-auc:0.60063+0.00880\n",
      "[90]\ttrain-auc:0.65193+0.00161\ttest-auc:0.60050+0.00896\n",
      "[100]\ttrain-auc:0.65656+0.00188\ttest-auc:0.60039+0.00879\n",
      "[110]\ttrain-auc:0.66111+0.00187\ttest-auc:0.59982+0.00864\n",
      "[120]\ttrain-auc:0.66550+0.00195\ttest-auc:0.59986+0.00850\n",
      "[126]\ttrain-auc:0.66784+0.00208\ttest-auc:0.59972+0.00859\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, d_train, num_boost_round=1000, verbose_eval=10, nfold=5, metrics=['auc'],\n",
    "     early_stopping_rounds=50, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae2979e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2472cb6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=None, booster=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, enable_categorical=False, eta=0.05,\n",
       "              eval_metric='auc', gamma=None, gpu_id=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None,\n",
       "              max_delta_step=None, max_depth=4, min_child_weight=4, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=50, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
       "              subsample=0.7, tree_method=None, validate_parameters=None, ...)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "046237c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/harry/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              enable_categorical=False, eta=0.05, eval_metric='auc', gamma=0,\n",
       "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.0500000007, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=4, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=36, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0.00499999989, reg_lambda=1,\n",
       "              scale_pos_weight=1, subsample=0.7, tree_method='exact',\n",
       "              validate_parameters=1, ...)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(x_train, y_train, eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "216c8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb_topic_hash_avg_author_score.csv'\n",
    "y_pred = xgb_model.predict_proba(x_test)[:,1]\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69788e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.concatenate([\n",
    "# #                             train_topic_tfidf.toarray(), \n",
    "# #                             train_title_tfidf.toarray(), \n",
    "# #                           train_content_tfidf.toarray(),\n",
    "\n",
    "                            \n",
    "# #                         train_content_svd,\n",
    "#                             train_topic_hash.toarray(), \n",
    "# #                             train_title_hash.toarray(),\n",
    "# #                             train_content_hash.toarray(),\n",
    "\n",
    "#                           train_ohe_channel,\n",
    "#                           train_ohe_weekday,\n",
    "#                           train_ohe_author, \n",
    "#                           np.expand_dims(df_train['is_weekend'].values, axis=-1),\n",
    "#                           np.expand_dims(df_train['img count'].values, axis=-1),\n",
    "#                           np.expand_dims(df_train['media count'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_tokens_title'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_tokens_content'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_non_stop_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['num_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['num_self_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['day_of_month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['hour'].values, axis=-1),\n",
    "# #                             sentiment\n",
    "#                             np.expand_dims(df_train['global_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['global_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['title_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['abs_title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['abs_title_sentiment_polarity'].values, axis=-1),\n",
    "# #                                 word sentiment\n",
    "#                             np.expand_dims(df_train['rate_positive_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['rate_negative_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['avg_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['min_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['max_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['avg_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['min_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['max_negative_polarity'].values, axis=-1),\n",
    "\n",
    "\n",
    "#                             ], axis=1)\n",
    "\n",
    "# x_test = np.concatenate([\n",
    "# #                             test_topic_tfidf.toarray(), \n",
    "# #                             test_title_tfidf.toarray(), \n",
    "# #                           test_content_tfidf.toarray(),\n",
    "    \n",
    "# #                         test_content_svd,\n",
    "#                             test_topic_hash.toarray(), \n",
    "# #                             test_title_hash.toarray(),\n",
    "# #                             test_content_hash.toarray(),\n",
    "\n",
    "#                           test_ohe_channel,\n",
    "#                           test_ohe_weekday,\n",
    "#                           test_ohe_author, \n",
    "#                           np.expand_dims(df_test['is_weekend'].values, axis=-1),\n",
    "#                           np.expand_dims(df_test['img count'].values, axis=-1),\n",
    "#                           np.expand_dims(df_test['media count'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_tokens_title'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_tokens_content'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_non_stop_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['num_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['num_self_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['day_of_month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['hour'].values, axis=-1),\n",
    "# #                             sentiment\n",
    "#                             np.expand_dims(df_test['global_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['global_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['title_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['abs_title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['abs_title_sentiment_polarity'].values, axis=-1),\n",
    "# #                                 word sentiment\n",
    "#                             np.expand_dims(df_test['rate_positive_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['rate_negative_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['avg_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['min_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['max_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['avg_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['min_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['max_negative_polarity'].values, axis=-1),\n",
    "#                             ], axis=1)\n",
    "\n",
    "# y_train = df_train['Popularity'].to_numpy()\n",
    "# y_train[y_train==-1] = 0\n",
    "\n",
    "# print(x_train.shape, y_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc997472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "#               'learning_rate': stats.uniform(0.01, 0.07),\n",
    "#               'subsample': stats.uniform(0.3, 0.7),\n",
    "#               'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "#               'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "#               'min_child_weight': [1, 2, 3]\n",
    "#              }\n",
    "\n",
    "# fit_params = {\n",
    "#     'eval_metric': \"auc\",\n",
    "#     'early_stopping_rounds': 50,\n",
    "#     'verbose' :  True,\n",
    "#     'objective' :'binary:logistic'\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(**fit_params)\n",
    "# clf = RandomizedSearchCV(xgb_model, \n",
    "#                          param_distributions = param_dist, \n",
    "#                          n_iter = 20, \n",
    "#                          scoring='roc_auc', \n",
    "#                          verbose = 3, \n",
    "#                          cv=kfold,\n",
    "#                          n_jobs = 2, refit=True)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_metric': \"auc\",\n",
    "    'early_stopping_rounds': 30,\n",
    "    'verbose' :  True,\n",
    "    'objective' :'binary:logistic'\n",
    "}\n",
    "xgb_model = xgb.XGBClassifier(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "model = xgb.train(xgb_params, d_train, 2000, watchlist, verbose_eval=10, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb.csv'\n",
    "y_pred = model.predict(d_test)\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbab91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88820dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "# get sort indices in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            [indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa568c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94173b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab070daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a706ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3c5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
