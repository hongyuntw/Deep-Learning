{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from summa import keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf59f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "11847\n",
      "39490\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_feature.csv')\n",
    "df_test = pd.read_csv('./test_feature.csv')\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177716ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['is_weekend'] = df_train['weekday'].apply(lambda x: 1 if x ==' Sat' or x == ' Sun' else 0)\n",
    "df_test['is_weekend'] = df_test['weekday'].apply(lambda x: 1 if x ==' Sat' or x == ' Sun' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1be0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # month\n",
    "# def pop_month(x):\n",
    "#     if x == 3:\n",
    "#         return 1\n",
    "#     elif x == 10:\n",
    "#         return -1 # -1 means not popular\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_month'] = df_train['month'].apply(pop_month)\n",
    "# df_test['popular_month'] = df_test['month'].apply(pop_month)\n",
    "# df_train.corr()['popular_month']['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733e9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hours\n",
    "# def pop_hour(x):\n",
    "#     if x == 13 or x == 21:\n",
    "#         return -1\n",
    "#     elif x == 5:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_hour'] = df_train['hour'].apply(pop_hour)\n",
    "# df_test['popular_hour'] = df_test['hour'].apply(pop_hour)\n",
    "# df_train.corr()['popular_hour']['Popularity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc6c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # channel \n",
    "# # just watch the EDA and assign weights\n",
    "# def pop_channel(x):\n",
    "#     if x == 'social-media' or x == 'tech':\n",
    "#         return 2\n",
    "#     elif x == 'marketing' or x == 'lifestyle':\n",
    "#         return 1\n",
    "#     elif x == 'world' or x == 'entertainment':\n",
    "#         return -2\n",
    "#     elif x == 'business':\n",
    "#         return -1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# df_train['popular_channel'] = df_train['channel'].apply(pop_channel)\n",
    "# df_test['popular_channel'] = df_test['hour'].apply(pop_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8151617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_popularity_author(col):\n",
    "#     df = df_train.groupby(f'{col}').mean().reset_index().sort_values(by='Popularity', ascending=False) \\\n",
    "#               [[f'{col}', 'Popularity']]\n",
    "#     df.columns=[f'{col}', 'avg_popularity']\n",
    "    \n",
    "#     '''\n",
    "#     pop_5 = df[df['avg_popularity'] >= 0.7][f'{col}'].values\n",
    "#     pop_4 = df[(df['avg_popularity'] < 0.7) & (df['avg_popularity'] >= 0.6)][f'{col}'].values\n",
    "#     pop_3 = df[(df['avg_popularity'] < 0.6) & (df['avg_popularity'] >= 0.5)][f'{col}'].values\n",
    "#     pop_2 = df[(df['avg_popularity'] < 0.5) & (df['avg_popularity'] >= 0.4)][f'{col}'].values\n",
    "#     pop_1 = df[(df['avg_popularity'] < 0.4) & (df['avg_popularity'] >= 0.3)][f'{col}'].values\n",
    "#     pop_0 = df[df['avg_popularity'] < 0.3][f'{col}'].values\n",
    "#     '''\n",
    "#     pop_5 = df[df['avg_popularity'] >= 0.5][f'{col}'].values\n",
    "#     pop_2 = df[(df['avg_popularity'] >= 0.2) & (df['avg_popularity'] < 0.5)][f'{col}'].values\n",
    "#     unpop_2 = df[(df['avg_popularity'] <= -0.2) & (df['avg_popularity'] >= -0.5)][f'{col}'].values\n",
    "#     unpop_5 = df[df['avg_popularity'] < -0.5][f'{col}'].values\n",
    "    \n",
    "#     def lambda_fxn(x):\n",
    "#         '''\n",
    "#         if x in pop_5:\n",
    "#             return 5\n",
    "#         elif x in pop_4:\n",
    "#             return 4\n",
    "#         elif x in pop_3:\n",
    "#             return 3\n",
    "#         elif x in pop_2:\n",
    "#             return 2\n",
    "#         elif x in pop_1:\n",
    "#             return 1\n",
    "#         elif x in pop_0:\n",
    "#             return -1\n",
    "            \n",
    "#         # To catch news desks/sections/subsections/material in test but not in train\n",
    "#         else:\n",
    "#             return 0\n",
    "#         '''\n",
    "        \n",
    "#         if x in pop_5:\n",
    "#             return 5\n",
    "#         elif x in pop_2:\n",
    "#             return 2\n",
    "#         elif x in unpop_5:\n",
    "#             return -5\n",
    "#         elif x in unpop_2:\n",
    "#             return -2\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "    \n",
    "#     df_train[f'popular_{col}'] = df_train[f'{col}'].apply(lambda_fxn)\n",
    "#     df_test[f'popular_{col}'] = df_test[f'{col}'].apply(lambda_fxn)\n",
    "# map_popularity_author('author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0f6616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 19 Jun 2013 15:04:30 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>There may be killer asteroids headed for Eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.153571</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>asteroid mission earth nasa said think idea se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 28 Mar 2013 17:40:55 +0000</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Google took a stand of sorts against patent-l...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>googl open patent opensourc softwar mapreduc m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 07 May 2014 19:15:20 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>You've spend countless hours training to be a...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.433992</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>fail olymp summer select nfl draft dunk celebr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                              topic  \\\n",
       "0   0          -1  Asteroid Asteroids challenge Earth Space U.S. ...   \n",
       "1   1           1  Apps and Software Google open source opn pledg...   \n",
       "2   2           1      Entertainment NFL NFL Draft Sports Television   \n",
       "\n",
       "         channel weekday                         pub_date            author  \\\n",
       "0          world     Wed  Wed, 19 Jun 2013 15:04:30 +0000               NaN   \n",
       "1           tech     Thu  Thu, 28 Mar 2013 17:40:55 +0000  Christina Warren   \n",
       "2  entertainment     Wed  Wed, 07 May 2014 19:15:20 +0000         Sam Laird   \n",
       "\n",
       "   img count                                              title  \\\n",
       "0          1  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1          2  Google's New Open Source Patent Pledge: We Won...   \n",
       "2          2  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "\n",
       "                                             content  ...  \\\n",
       "0   There may be killer asteroids headed for Eart...  ...   \n",
       "1   Google took a stand of sorts against patent-l...  ...   \n",
       "2   You've spend countless hours training to be a...  ...   \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.136364                    0.8              -0.153571   \n",
       "1               0.136364                    0.7              -0.130000   \n",
       "2               0.062500                    1.0              -0.433992   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  day_of_month  month  hour  \\\n",
       "0                  -0.25                 -0.125            19      6    15   \n",
       "1                  -0.25                 -0.050            28      3    17   \n",
       "2                  -1.00                 -0.050             7      5    19   \n",
       "\n",
       "   is_weekend                                           keywords  \n",
       "0           0  asteroid mission earth nasa said think idea se...  \n",
       "1           0  googl open patent opensourc softwar mapreduc m...  \n",
       "2           0  fail olymp summer select nfl draft dunk celebr...  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6760a8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# define extra stopwords\n",
    "extra_stopwords = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
    "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
    "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
    "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
    "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
    "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
    "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
    "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
    "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
    "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
    "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
    "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
    "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
    "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
    "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
    "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
    "                    \"you've\", \"'s\", \"'d\", \"'m\", \"abov\", \"afterward\", \"ai\", \"alon\", \"alreadi\", \"alway\", \"ani\", \n",
    "                     \"anoth\", \"anyon\", \"anyth\", \"anywher\", \"becam\", \"becaus\", \"becom\", \"befor\", \n",
    "                     \"besid\", \"ca\", \"cri\", \"dare\", \"describ\", \"did\", \"doe\", \"dure\", \"els\", \n",
    "                     \"elsewher\", \"empti\", \"everi\", \"everyon\", \"everyth\", \"everywher\", \"fifti\", \n",
    "                     \"forti\", \"gon\", \"got\", \"henc\", \"hereaft\", \"herebi\", \"howev\", \"hundr\", \"inde\", \n",
    "                     \"let\", \"ll\", \"mani\", \"meanwhil\", \"moreov\", \"n't\", \"na\", \"need\", \"nobodi\", \"noon\", \n",
    "                     \"noth\", \"nowher\", \"ol\", \"onc\", \"onli\", \"otherwis\", \"ought\", \"ourselv\", \"perhap\", \n",
    "                     \"pleas\", \"sever\", \"sha\", \"sinc\", \"sincer\", \"sixti\", \"somebodi\", \"someon\", \"someth\", \n",
    "                     \"sometim\", \"somewher\", \"ta\", \"themselv\", \"thenc\", \"thereaft\", \"therebi\", \"therefor\", \n",
    "                     \"togeth\", \"twelv\", \"twenti\", \"ve\", \"veri\", \"whatev\", \"whenc\", \"whenev\", \n",
    "                    \"wherea\", \"whereaft\", \"wherebi\", \"wherev\", \"whi\", \"wo\", \"anywh\", \"el\", \"elsewh\", \"everywh\", \n",
    "                    \"ind\", \"otherwi\", \"plea\", \"somewh\", \"yourselv\"]\n",
    "\n",
    "stop = stop + extra_stopwords\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wnl = WordNetLemmatizer()\n",
    "def prep(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [porter.stem(w) for w in tokens if w not in stop and w.isalpha()]\n",
    "    return tokens\n",
    "#     return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "#             if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d087d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "# df_train['keywords'] = df_train['content'].progress_apply(prep) # 此步驟約要花五分鐘\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(tokenize)\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(lambda x: ' '.join(x))\n",
    "# df_train['keywords'] = df_train['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f9be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('./train_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c281a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "# df_test['keywords'] = df_test['content'].progress_apply(prep) # 此步驟約要花五分鐘\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(tokenize)\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(lambda x: ' '.join(x))\n",
    "# df_test['keywords'] = df_test['keywords'].progress_apply(lambda x: keywords.keywords(x).replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2e2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.to_csv('./test_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4059b93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'global_sentiment_polarity', 'global_subjectivity',\n",
       "       'title_subjectivity', 'title_sentiment_polarity',\n",
       "       'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'day_of_month', 'month', 'hour', 'is_weekend',\n",
       "       'keywords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2006f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topic = df_train['topic'].values.astype('U').tolist() + df_test['topic'].values.astype('U').tolist()\n",
    "all_channel = df_train['channel'].values.astype('U').tolist() + df_test['channel'].values.astype('U').tolist()\n",
    "all_titles = df_train['title'].values.astype('U').tolist() + df_test['title'].values.astype('U').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b458f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents = list(df_train['content'].values) \n",
    "test_contents = list(df_test['content'].values)\n",
    "all_contents = train_contents + test_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1452a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 5000)\n",
      "(11847, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  \n",
    "                                  tokenizer=tokenize,\n",
    "                                  max_features = 5000,\n",
    "                                  dtype = np.float32)\n",
    "\n",
    "train_keywords_tfidf = tfidf_vectorize.fit_transform(df_train['keywords'].values.astype('U').tolist())\n",
    "test_keywords_tfidf = tfidf_vectorize.transform(df_test['keywords'].values.astype('U').tolist())\n",
    "print(train_keywords_tfidf.shape)\n",
    "print(test_keywords_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3baf0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaliyah', 'aaron', 'abandon', 'abbey', 'abbott', 'abc', 'abduct', 'abil', 'abl', 'aboard', 'abram', 'abramson', 'abroad', 'absolut', 'absorb', 'absurd', 'abu', 'academ', 'academi', 'accel', 'accent', 'accept', 'access', 'accessori', 'accid', 'accompani', 'accomplish', 'accord', 'account', 'accu', 'accur', 'accuraci', 'acer', 'achiev', 'acknowledg', 'aclu', 'acquir', 'acquisit', 'act', 'action', 'activ', 'activi', 'activist', 'actor', 'actress', 'actual', 'ad', 'adam', 'adapt', 'add', 'addict', 'addit', 'address', 'adida', 'adjust', 'administr', 'admir', 'admit', 'adob', 'adopt', 'ador', 'adult', 'advanc', 'advantag', 'adventur', 'adverti', 'advi', 'advic', 'advoc', 'aereo', 'aerial', 'aesthet', 'affair', 'affect', 'affleck', 'afford', 'afghan', 'afghanistan', 'afraid', 'africa', 'african', 'afterlight', 'afternoon', 'age', 'agenc', 'agent', 'aggreg', 'aggress', 'ago', 'agr', 'agreement', 'agricultur', 'ahead', 'aid', 'aim', 'air', 'airbnb', 'aircraft', 'airlin', 'airplan']\n"
     ]
    }
   ],
   "source": [
    "vocab  = tfidf_vectorize.get_feature_names()\n",
    "print(vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8711004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  \n",
    "#                                   tokenizer=tokenize,\n",
    "#                                   ngram_range=(1,1), \n",
    "#                                   sublinear_tf = True,\n",
    "# #                                   max_features = 2048,\n",
    "#                                   dtype = np.float32)\n",
    "# all_content_tfidf = tfidf_vectorize.fit_transform(all_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecbebcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_content_tfidf = tfidf_vectorize.transform(train_contents)\n",
    "# test_content_tfidf = tfidf_vectorize.transform(test_contents)\n",
    "# print(train_content_tfidf.shape)\n",
    "# print(test_content_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be097792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD, PCA\n",
    "# svd = TruncatedSVD(n_components=100)\n",
    "# svd.fit(all_content_tfidf)\n",
    "\n",
    "# train_content_svd = svd.transform(train_content_tfidf)\n",
    "# test_content_svd = svd.transform(test_content_tfidf)\n",
    "\n",
    "# print(train_content_svd.shape)\n",
    "# print(test_content_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b0ab332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_topic_tfidf = tfidf_vectorize.transform(df_train['topic'].values.astype('U').tolist())\n",
    "# test_topic_tfidf = tfidf_vectorize.transform(df_test['topic'].values.astype('U').tolist())\n",
    "\n",
    "# print(train_topic_tfidf.shape)\n",
    "# print(test_topic_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f51f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_title_tfidf = tfidf_vectorize.transform(df_train['title'].values.astype('U').tolist())\n",
    "# test_title_tfidf = tfidf_vectorize.transform(df_test['title'].values.astype('U').tolist())\n",
    "\n",
    "# print(train_title_tfidf.shape)\n",
    "# print(test_title_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea828093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hashvec = HashingVectorizer(n_features=1024,\n",
    "#                             preprocessor=prep,\n",
    "#                             tokenizer=tokenize,\n",
    "#                            dtype = np.float32)\n",
    "\n",
    "# train_topic_hash = hashvec.fit_transform(df_train['topic'].values.astype('U').tolist())\n",
    "# test_topic_hash = hashvec.transform(df_test['topic'].values.astype('U').tolist())\n",
    "# print(train_topic_hash.shape)\n",
    "# print(test_topic_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfddbc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hashvec = HashingVectorizer(n_features=1024,\n",
    "#                            dtype = np.float32)\n",
    "\n",
    "# train_keywords_hash = hashvec.fit_transform(df_train['keywords'].values.astype('U').tolist())\n",
    "# test_keywords_hash = hashvec.transform(df_test['keywords'].values.astype('U').tolist())\n",
    "# print(train_keywords_hash.shape)\n",
    "# print(test_keywords_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5adb2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_content_hash = hashvec.fit_transform(train_contents)\n",
    "# test_content_hash = hashvec.transform(test_contents)\n",
    "# print(train_content_hash.shape)\n",
    "# print(test_content_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "012a14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_title_hash = hashvec.transform(df_train['title'].values.astype('U').tolist())\n",
    "# test_title_hash = hashvec.transform(df_test['title'].values.astype('U').tolist())\n",
    "# print(train_title_hash.shape)\n",
    "# print(test_title_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93edc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f740898c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 33)\n",
      "(11847, 33)\n"
     ]
    }
   ],
   "source": [
    "# ohe channel\n",
    "OHE_channel = OneHotEncoder(handle_unknown='ignore')\n",
    "train_ohe_channel = OHE_channel.fit_transform(df_train['channel'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_channel = OHE_channel.transform(df_test['channel'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_channel.shape)\n",
    "print(test_ohe_channel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93a465c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 7)\n",
      "(11847, 7)\n"
     ]
    }
   ],
   "source": [
    "# ohe weekday\n",
    "OHE_weekday = OneHotEncoder(handle_unknown='ignore')\n",
    "all_week_day = list(df_train['weekday'].values) +  list(df_test['weekday'].values)\n",
    "# OHE_weekday.fit(np.array(all_week_day).reshape(-1,1))\n",
    "\n",
    "train_ohe_weekday = OHE_weekday.fit_transform(df_train['weekday'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_weekday = OHE_weekday.transform(df_test['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_weekday.shape)\n",
    "print(test_ohe_weekday.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "055376c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 428)\n",
      "(11847, 428)\n"
     ]
    }
   ],
   "source": [
    "# ohe author\n",
    "OHE_author = OneHotEncoder(handle_unknown='ignore')\n",
    "all_author = list(df_train['author'].values) +  list(df_test['author'].values)\n",
    "# OHE_author.fit(np.array(all_author).reshape(-1,1))\n",
    "\n",
    "train_ohe_author = OHE_author.fit_transform(df_train['author'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_author = OHE_author.transform(df_test['author'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "print(train_ohe_author.shape)\n",
    "print(test_ohe_author.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9df222d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the numerical features\n",
    "# from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# # scaler = StandardScaler()\n",
    "# need_to_normalize = [\n",
    "#     'img count', \n",
    "#     'media count',\n",
    "#     'n_tokens_content',\n",
    "#     'n_tokens_title', \n",
    "#     'num_hrefs', \n",
    "#     'num_self_hrefs',\n",
    "# #     'day_of_month',\n",
    "# #     'month',\n",
    "# #     'hour',\n",
    "# #     'rate_positive_words',\n",
    "# #     'rate_negative_words',\n",
    "# #     'avg_positive_polarity',\n",
    "# #     'avg_negative_polarity',\n",
    "# #     'max_positive_polarity',\n",
    "# #     'min_positive_polarity',\n",
    "# #     'max_negative_polarity',\n",
    "# #     'min_negative_polarity',\n",
    "# #     'abs_title_subjectivity',\n",
    "# #     'abs_title_sentiment_polarity',\n",
    "# #     'popular_month',\n",
    "# #     'popular_hour',\n",
    "# #     'popular_channel',\n",
    "# #     'popular_author',\n",
    "#     ]\n",
    "\n",
    "# df_train_normalize = df_train.copy(deep=True)\n",
    "# df_test_normalize = df_test.copy(deep=True)\n",
    "\n",
    "# # scaler.fit(df_all[numerical])\n",
    "# df_train_normalize[need_to_normalize] = scaler.fit_transform(df_train[need_to_normalize])\n",
    "# df_test_normalize[need_to_normalize] = scaler.transform(df_test[need_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "259c9d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'global_sentiment_polarity', 'global_subjectivity',\n",
       "       'title_subjectivity', 'title_sentiment_polarity',\n",
       "       'abs_title_subjectivity', 'abs_title_sentiment_polarity',\n",
       "       'rate_positive_words', 'rate_negative_words', 'avg_positive_polarity',\n",
       "       'min_positive_polarity', 'max_positive_polarity',\n",
       "       'avg_negative_polarity', 'min_negative_polarity',\n",
       "       'max_negative_polarity', 'day_of_month', 'month', 'hour', 'is_weekend',\n",
       "       'keywords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "869cf72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 37)\n",
      "(27643, 5496)\n"
     ]
    }
   ],
   "source": [
    "df_train_concat = pd.concat([df_train[['Popularity',\n",
    "                                                'img count', \n",
    "                                                'media count',\n",
    "                                                'n_tokens_title',\n",
    "                                                'n_tokens_content',\n",
    "                                                'n_unique_tokens',\n",
    "                                                'n_non_stop_words',\n",
    "                                                'n_non_stop_unique_tokens',\n",
    "                                                'num_hrefs',\n",
    "                                                'num_self_hrefs',\n",
    "                                                'global_sentiment_polarity',\n",
    "                                                'global_subjectivity',\n",
    "                                                'title_subjectivity',\n",
    "                                                'title_sentiment_polarity',\n",
    "                                                'abs_title_subjectivity',\n",
    "                                                'abs_title_sentiment_polarity',\n",
    "                                                'rate_positive_words',\n",
    "                                                'rate_negative_words',\n",
    "                                                'avg_positive_polarity',\n",
    "                                                'min_positive_polarity',\n",
    "                                                'max_positive_polarity',\n",
    "                                                'avg_negative_polarity',\n",
    "                                                'min_negative_polarity',\n",
    "                                                'max_negative_polarity',\n",
    "                                                'day_of_month',\n",
    "                                                'month',\n",
    "                                                'hour',\n",
    "                                                'is_weekend',\n",
    "                                               ]], \n",
    "                      pd.DataFrame(train_keywords_tfidf.toarray()),\n",
    "                      pd.DataFrame(train_ohe_channel, columns=OHE_channel.get_feature_names()),\n",
    "                      pd.DataFrame(train_ohe_weekday, columns=OHE_weekday.get_feature_names()),\n",
    "                      pd.DataFrame(train_ohe_author,  columns=OHE_author.get_feature_names()),\n",
    "                     ], axis=1)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37700187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 36)\n",
      "(11847, 5495)\n"
     ]
    }
   ],
   "source": [
    "df_test_concat = pd.concat([df_test[['img count', \n",
    "                                                'media count',\n",
    "                                                'n_tokens_title',\n",
    "                                                'n_tokens_content',\n",
    "                                                'n_unique_tokens',\n",
    "                                                'n_non_stop_words',\n",
    "                                                'n_non_stop_unique_tokens',\n",
    "                                                'num_hrefs',\n",
    "                                                'num_self_hrefs',\n",
    "                                                'global_sentiment_polarity',\n",
    "                                                'global_subjectivity',\n",
    "                                                'title_subjectivity',\n",
    "                                                'title_sentiment_polarity',\n",
    "                                                'abs_title_subjectivity',\n",
    "                                                'abs_title_sentiment_polarity',\n",
    "                                                'rate_positive_words',\n",
    "                                                'rate_negative_words',\n",
    "                                                'avg_positive_polarity',\n",
    "                                                'min_positive_polarity',\n",
    "                                                'max_positive_polarity',\n",
    "                                                'avg_negative_polarity',\n",
    "                                                'min_negative_polarity',\n",
    "                                                'max_negative_polarity',\n",
    "                                                'day_of_month',\n",
    "                                                'month',\n",
    "                                                'hour',\n",
    "                                                'is_weekend',\n",
    "                                               ]], \n",
    "                      pd.DataFrame(test_keywords_tfidf.toarray()),\n",
    "                      pd.DataFrame(test_ohe_channel, columns=OHE_channel.get_feature_names()),\n",
    "                      pd.DataFrame(test_ohe_weekday, columns=OHE_weekday.get_feature_names()),\n",
    "                      pd.DataFrame(test_ohe_author,  columns=OHE_author.get_feature_names()),\n",
    "                     ], axis=1)\n",
    "\n",
    "print(df_test.shape)\n",
    "print(df_test_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28424861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>img count</th>\n",
       "      <th>media count</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>...</th>\n",
       "      <th>x0_Wink</th>\n",
       "      <th>x0_Yalda Mostajeran</th>\n",
       "      <th>x0_Yelena Shuster</th>\n",
       "      <th>x0_Yohana Desta</th>\n",
       "      <th>x0_Zach Cutler</th>\n",
       "      <th>x0_Zach Supalla</th>\n",
       "      <th>x0_Zachary Sniderman</th>\n",
       "      <th>x0_Ziv Eliraz</th>\n",
       "      <th>x0_Zoe Fox</th>\n",
       "      <th>x0_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>575</td>\n",
       "      <td>0.507826</td>\n",
       "      <td>0.615652</td>\n",
       "      <td>0.424348</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>305</td>\n",
       "      <td>0.478689</td>\n",
       "      <td>0.619672</td>\n",
       "      <td>0.357377</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>1119</td>\n",
       "      <td>0.488829</td>\n",
       "      <td>0.638963</td>\n",
       "      <td>0.421805</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>274</td>\n",
       "      <td>0.711679</td>\n",
       "      <td>0.733577</td>\n",
       "      <td>0.583942</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1370</td>\n",
       "      <td>0.497080</td>\n",
       "      <td>0.756934</td>\n",
       "      <td>0.464964</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5496 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Popularity  img count  media count  n_tokens_title  n_tokens_content  \\\n",
       "0          -1          1            0               8               575   \n",
       "1           1          2            0              12               305   \n",
       "2           1          2           25              12              1119   \n",
       "3          -1          1           21               5               274   \n",
       "4          -1         52            1              10              1370   \n",
       "\n",
       "   n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  \\\n",
       "0         0.507826          0.615652                  0.424348         22   \n",
       "1         0.478689          0.619672                  0.357377         18   \n",
       "2         0.488829          0.638963                  0.421805         11   \n",
       "3         0.711679          0.733577                  0.583942         13   \n",
       "4         0.497080          0.756934                  0.464964         16   \n",
       "\n",
       "   num_self_hrefs  ...  x0_Wink  x0_Yalda Mostajeran  x0_Yelena Shuster  \\\n",
       "0               0  ...      0.0                  0.0                0.0   \n",
       "1               3  ...      0.0                  0.0                0.0   \n",
       "2               4  ...      0.0                  0.0                0.0   \n",
       "3               6  ...      0.0                  0.0                0.0   \n",
       "4               7  ...      0.0                  0.0                0.0   \n",
       "\n",
       "   x0_Yohana Desta  x0_Zach Cutler  x0_Zach Supalla  x0_Zachary Sniderman  \\\n",
       "0              0.0             0.0              0.0                   0.0   \n",
       "1              0.0             0.0              0.0                   0.0   \n",
       "2              0.0             0.0              0.0                   0.0   \n",
       "3              0.0             0.0              0.0                   0.0   \n",
       "4              0.0             0.0              0.0                   0.0   \n",
       "\n",
       "   x0_Ziv Eliraz  x0_Zoe Fox  x0_nan  \n",
       "0            0.0         0.0     1.0  \n",
       "1            0.0         0.0     0.0  \n",
       "2            0.0         0.0     0.0  \n",
       "3            0.0         0.0     0.0  \n",
       "4            0.0         0.0     0.0  \n",
       "\n",
       "[5 rows x 5496 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = abs(df_train_concat._get_numeric_data().corr()['Popularity']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6c736b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971\n"
     ]
    }
   ],
   "source": [
    "# keep_cols = []\n",
    "# th = 0.004\n",
    "# for index, value in feats.items():\n",
    "# #     print(f\"Index : {index} \\t\\t, Value : {value}\")\n",
    "#     if value > th and index != 'Popularity':\n",
    "#         keep_cols.append(index)\n",
    "# print(len(keep_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cc290919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Popularity'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_cols = list(feats.keys())\n",
    "keep_cols = keep_cols[:1001]\n",
    "keep_cols.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "be78fc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is_weekend', 'month', 'x0_ Sat', 'x0_ Sun', 'x0_Anita Li', 'x0_ Wed', 'x0_Alex Fitzpatrick', 'x0_world', 'x0_ Tue', 'x0_ Fri', 'x0_social-media', 316, 'num_hrefs', 507, 'x0_ Thu', 524, 586, 128, 'x0_Matt Petronzio', 'x0_Sam Laird', 381, 'x0_marketing', 1011, 354, 'global_subjectivity', 'x0_Rebecca Hiscott', 'n_non_stop_unique_tokens', 'avg_positive_polarity', 'x0_Charlie White', 'max_positive_polarity', 175, 574, 1021, 525, 533, 435, 477, 163, 'n_tokens_title', 9, 'x0_ Mon', 515, 1000, 460, 'x0_lifestyle', 'n_unique_tokens', 'x0_Allison Reiber', 'global_sentiment_polarity', 897, 'x0_Camille Bautista', 755, 'x0_Bob Al-Greene', 'n_tokens_content', 317, 864, 334, 'x0_Chris Taylor', 255, 1001, 69, 270, 27, 'n_non_stop_words', 953, 'hour', 626, 'x0_Quora', 3, 573, 936, 'title_sentiment_polarity', 'x0_Eric Larson', 275, 'x0_Alex Hazlett', 750, 1004, 'rate_positive_words', 662, 776, \"x0_James O'Brien\", 243, 'x0_sports', 'x0_Yohana Desta', 'x0_Monty Munford', 905, 505, 833, 542, 426, 'x0_tech', 'x0_Lauren Indvik', 51, 'x0_Sharlyn Lauby', 'x0_Josh Tolan', 869, 478, 'x0_Aubre Andrus', 191, 843, 977, 834, 472, 397, 964, 898, 124, 185, 832, 274, 677, 570, 'x0_Jennifer Parris', 612, 643, 753, 566, 'x0_Rex Santus', 218, 885, 'x0_apps-software', 'x0_Stephanie Buck', 'x0_Bob Garfield', 'x0_Veena Bissram', 127, 'x0_entertainment', 812, 559, 87, 'x0_Paul Laudiero', 459, 949, 901, 367, 165, 139, 'x0_Madeline Raynor', 147, 803, 41, 61, 919, 336, 655, 348, 504, 160, 33, 311, 699, 'x0_Dani Fankhauser', 355, 676, 358, 223, 710, 797, 761, 1009, 152, 'x0_Scott Steinberg', 1018, 959, 'x0_ Amy-Mae Turner', 762, 942, 211, 'x0_Zoe Fox', 78, 819, 868, 668, 'x0_Sandra Gonzalez', 799, 57, 878, 168, 391, 637, 267, 508, 'x0_Natali Morris', 212, 248, 'x0_Koromone Koroye', 664, 'x0_media', 571, 835, 'x0_Emily Chow', 562, 'x0_Emily Price', 153, 433, 546, 428, 912, 718, 768, 543, 614, 'x0_Kari Paul', 17, 52, 731, 76, 838, 7, 530, 654, 871, 26, 407, 395, 558, 60, 65, 'x0_Eva Recinos', 736, 'day_of_month', 201, 807, 298, 184, 'x0_Matt Silverman', 708, 235, 'x0_Christine Erickson', 532, 581, 691, 'x0_Eamonn Carey', 'x0_Meghan Uno', 'x0_Contently', 'x0_Christina Ascani', 169, 948, 'x0_Lauren Drell', 'img count', 310, 531, 517, 182, 81, 'x0_Corinne Bagish', 825, 68, 748, 'x0_Scott Gerber', 1012, 692, 'x0_Robin Raskin', 'x0_Mike Volpe', 550, 'x0_startups', 480, 394, 606, 944, 983, 377, 156, 231, 'x0_Danielle Odiamar', 'avg_negative_polarity', 'x0_Stewart Wolpin', 937, 37, 'x0_Kate Sommers-Dawes', 'x0_Kristen Kampetis', 820, 79, 436, 292, 876, 'x0_Will Fenstermaker', 772, 584, 989, 268, 'x0_T.L. Stanley', 259, 250, 'x0_Matt Schneiderman', 'num_self_hrefs', 806, 781, 714, 'x0_Dennis Green', 556, 854, 523, 808, 470, 958, 458, 'x0_Seth Fiegerman', 'x0_Sheffie Elencweig', 29, 244, 376, 607, 941, 273, 125, 107, 'x0_Connor Finnegan', 715, 602, 74, 210, 324, 103, 856, 499, 484, 711, 373, 877, 86, 502, 188, 289, 759, 'abs_title_sentiment_polarity', 'x0_Adam Popescu', 173, 910, 860, 444, 441, 365, 782, 817, 555, 'x0_business', 264, 'x0_Brian Heater', 767, 239, 257, 236, 326, 769, 143, 639, 979, 172, 284, 'x0_AJ Marechal', 745, 388, 11, 'x0_socmed', 'x0_Jason Stanley', 'x0_Meghan Peters', 'x0_Max Blau', 'x0_memes', 'x0_Ashley Mosley', 'x0_Esurance', 'x0_Rocco Sannelli', 'x0_Brian Solis', 'x0_Gil Dudkiewicz', 'x0_Chelsea Gladden', 'x0_Amy Burke', 'x0_Eric Grafstrom', 'x0_Zach Cutler', 'x0_mob', 'x0_Ryan Holiday', 686, 1015, 1002, 6, 449, 933, 851, 18, 611, 539, 186, 603, 822, 'x0_Elisha Hartwig', 'x0_Visa', 'x0_Ritika Trikha', 'x0_Robyn Peterson', 'x0_Kaitlyn Jakola', 'x0_Frederic Kerrest', 'x0_Dan Tynan', 'x0_Nissan', 'x0_Tom Wentworth', 'x0_Todd Neff', 'x0_Mark Newman', 917, 621, 641, 424, 974, 513, 985, 'x0_Jesse Draper', 'x0_Heather Huhman', 'x0_Josh Catone', 'x0_John C. Havens', 'x0_Alexis Grant', 'x0_Gloria Dawson', 'x0_Travis Andrews', 991, 213, 925, 437, 773, 903, 180, 94, 634, 12, 230, 343, 97, 815, 597, 162, 'x0_Lindsay Rothfeld', 'min_negative_polarity', 595, 476, 836, 362, 'x0_Hannah Orenstein', 696, 618, 'x0_Brent Butterworth', 'x0_Tracey Edouard', 585, 'x0_Jessica Catcher', 288, 206, 'x0_Jenni Ryall', 783, 590, 895, 99, 157, 617, 202, 281, 'x0_Tracey Wallace', 262, 410, 501, 30, 238, 1017, 309, 902, 630, 579, 190, 1020, 746, 140, 870, 349, 520, 403, 704, 129, 'x0_Sylvan Lane', 'x0_Blathnaid Healy', 522, 'x0_Kyli Singh', 93, 'x0_Dhiya Kuriakose', 544, 'x0_Nora Grenfell', 684, 552, 741, 475, 685, 625, 'x0_Dasha Battelle', 1022, 640, 635, 557, 197, 647, 466, 879, 'x0_gadgets', 663, 770, 957, 380, 984, 716, 672, 382, 982, 995, 986, 'x0_Jeremy Cabalona', 83, 108, 666, 'x0_Megan Hess', 446, 733, 'x0_Ashley Codianni', 938, 788, 777, 923, 13, 269, 791, 417, 967, 399, 146, 891, 171, 1, 'x0_nan', 452, 302, 19, 620, 867, 464, 622, 962, 'x0_Brian Ries', 589, 800, 'x0_Eli Epstein', 1003, 850, 194, 697, 'x0_Marian Barrett', 'x0_Jess Fee', 40, 915, 780, 792, 826, 'x0_dev-design', 'x0_Neha Prakash', 786, 249, 95, 649, 132, 'x0_Annie Colbert', 713, 363, 921, 49, 48, 890, 687, 'rate_negative_words', 916, 660, 351, 242, 737, 265, 875, 205, 113, 561, 613, 601, 465, 456, 495, 628, 'x0_The New York Times', 'x0_Brian Casel', 'x0_Ana Rosenstein', 84, 865, 934, 'x0_small-business', 431, 837, 196, 857, 978, 719, 216, 283, 340, 'x0_Cheri Warren', 'x0_Stacey Garratt', 'x0_Quaker', 'x0_Molly Socha', 'x0_Dave Kerpen', 'x0_Michael Surtees', 'x0_Nancy Friedman', 'x0_Mark Bussell', 'x0_comics', 'x0_Neal Ungerleider', 'x0_Dyson', 'x0_Jake Villarreal', 'x0_Naomi Mannino', 'x0_John Tabis', 'x0_Adora Svitak', 'x0_Peter Pomerantsev', 'x0_Aaron Miguel Cantú', 'x0_Nate Cooper', 'x0_Chen Amit', 'x0_Andre Lavoie', 'x0_Chris Gorham', 'x0_Ziv Eliraz', 'x0_Jonathan Ellis', 'x0_James Kenigsberg', 'x0_Bosch', 'x0_Tom Parker', 'x0_John Foreman', 'x0_The Home Depot', 'x0_Carolyn Wickware', 'x0_Jynwel Foundation', 'x0_Jamie Shupak', 'x0_Stacey Politi', 'x0_Michael Lebowitz', 'x0_Julie Halpert', 'x0_Stephane Le Viet', 'x0_viral', 'x0_Paul Spain', 'x0_Douglas Rushkoff', 'x0_Lauren Smith', 'x0_Elli Sharef', 'x0_Alan Danzis', 'x0_Sam Slaughter', 'x0_Jonah Berger', 'x0_Gloria Pitagorsky', 'x0_Melissa Cooper', 'x0_Conrad Wilson', 'x0_Ari Ratner', 'x0_Ryan Matzner', 'x0_Sam Ewen', 'x0_howto', 'x0_James Green', 'x0_James Mignano', 'x0_NewsCred', 'x0_Admiral James Stavridis', 'x0_Niki Walker', 'x0_Marriott', 'x0_Amy Webb', 'x0_Kevin Nakao', 'x0_Erin Bazinet', 'x0_Intel', 'x0_Sport Chek', 'x0_Jessica Randazza', 'x0_Lyssa Goldberg', 'x0_Brandon Smith', 'x0_Susan P. Joyce', 'x0_Dan Cohen', 'x0_Gravity', 'x0_Lola Akinmade Åkerström', 'x0_Jon Snow', 'x0_Brian Forde', 'x0_Sage', 'x0_Zach Supalla', 'x0_Evan Rudowski', 'x0_Ewan Spence', 'x0_Allison Kent-Smith', 'x0_Aaron Lee', 'x0_Kevin Galligan', 'x0_Zachary Sniderman', 'x0_Keith Messick', 'x0_Amit De', 'x0_Mark Newall', 'x0_Ryan Lytle', 44, 588, 333, 591, 338, 'max_negative_polarity', 89, 670, 'x0_Lance Ulanoff', 'x0_Kelly Meyers', 'x0_Holley Murchison', 'x0_Alex Howard', 'x0_Andrew Van Noy', 'x0_Sally Poblete', 'x0_Scott Pham', 'x0_#MAKEFURHISTORY', 'x0_Amit Chauhan', 'x0_Graham Cooke', 'x0_Robot Butler', 'x0_Holiday Inn', 'x0_Patrick Michels', 'x0_Mary Gay Townsend', 'x0_Brady Bohrmann', 'x0_Kenneth Hein', 'x0_Gary Shapiro', 'x0_Shane Richmond', 'x0_Hot Wheels', 'x0_Laura Tillman', 'x0_Julian Sanchez', 'x0_Sprint Business', 'x0_Claire Trageser', 'x0_Charlotte Phillips', 'x0_Philip Krim', 'x0_Stirling Cox', \"x0_Brian O'Kelley\", 'x0_Marni Usheroff', 'x0_Luke Bonner', 'x0_Susan Temple', 'x0_Kirk Baumann', 'x0_Wink', 'x0_Michael Rothman', 'x0_Manu Kumar', 'x0_Anna Parsons', 'x0_Duncan Madden', 'x0_Adelle Waldman', 'x0_Afifa Siddiqui', 'x0_Liz Kaplow', 'x0_Tamar Weinberg', 'x0_Victor Pineiro', 'x0_Jennifer Shore', 'x0_Yalda Mostajeran', 'x0_ConAgra', 'x0_Daniel Watson', 'x0_Lou Dorchen', 'x0_TREEbook', 'x0_Fred Mouawad', 'x0_Blair Hickman', 'x0_bus', 'x0_Mike Maples', 'x0_Jennifer Hanser', 'x0_Pete Cashmore', 'x0_Alex Thomson', 'x0_Bose', 'x0_Matt Straz', 'x0_Honeywell', 'x0_Yelena Shuster', 'x0_Brad Feld', 'x0_Jeffrey Bussgang', 'x0_Mark Warner', 'x0_Alex Torrenegra', 'x0_Rebecca Levey', 'x0_Beth Blecherman', 'x0_VTech', 'x0_Joshua Waldman', 'x0_Henry Timms', 'x0_Joe Carberry', 'x0_Ben Huh', 'x0_Macala Wright', 'x0_Bob Myhal', 56, 619, 526, 739, 149, 'x0_Jessica Plautz', 386, 102, 'x0_Trevor Gowan', 'x0_Jean Bentley', 'x0_Heather Martino', 'x0_Samsung Galaxy 4', 'x0_David Patrikarakos', 'x0_Seth Porges', 'x0_Jill Krasny', 'x0_Chelsea Stark', 301, 14, 927, 374, 256, 'x0_Emily Feldman', 'x0_Emily Siegel', 'x0_Bianca Consunji', 'x0_Nick Lavars', 322, 661, 482, 0, 100, 408, 296, 'x0_Samuel Axon', 887, 604, 805, 92, 577, 8, 135, 225, 675, 554, 987, 996, 627, 847, 829, 576, 813, 361, 842, 366, 159, 'x0_Patrick Gillespie', 318, 802, 1014, 681, 'x0_Eitan Levine', 608, 583, 575, 136, 785, 652, 422, 379, 680, 858, 'x0_Colin  Daileda', 973, 506, 683, 669, 587, 457, 519, 518, 'x0_Sara Afzal', 73, 'x0_Armand Valdes', 'x0_Rebecca Ruiz', 445, 55, 988, 990, 'x0_Lauren Hockenson', 145, 412, 971, 344, 187, 659, 161, 402, 418, 'x0_Adario Strange', 565, 534, 512, 883, 142, 572, 610, 174, 293, 742, 900, 'x0_Karissa Bell', 907, 166, 448, 707, 'x0_Luisa Rollenhagen', 975, 749, 120, 904, 841, 96, 673, 122, 623, 'x0_Jason Abbruzzese', 305, 'x0_Andrea Smith', 811, 'x0_The Associated Press', 28, 307, 72, 47, 144, 884, 'x0_Evan Engel', 'x0_Bing', 272, 'x0_Pete Pachal', 492, 479, 801, 237, 'x0_Brittany Levine Beckman', 'x0_how-to', 'x0_pics', 725, 892, 346, 'media count', 593, 385, 'x0_Anna Washenko', 'x0_music', 321, 899, 521, 764, 592, 'x0_Noah Sterling', 814, 347, 823, 383, 961, 117, 970, 193, 16, 447, 743, 'x0_Ann-Marie Alcantara', 118, 567, 50, 852, 116, 'x0_Megan Specia', 155, 'x0_film', 'x0_Samantha Dean', 631, 414, 330, 1016, 695, 'x0_Katie Sola', 657, 280, 232, 401, 404, 939, 859, 943, 726, 119, 'x0_Jordan Hoffman', 'x0_Raymond Wong', 'x0_Nina Frazier Hansen', 888, 151, 'x0_Alex Magdaleno', 20, 'x0_Mashable Video', 828, 'x0_Andrew Freedman', 709, 'x0_Andrea Romano', 370, 920, 'x0_Samsung', 'x0_Carrie Farler', 299, 420, 5, 130, 796, 53, 700, 246, 756, 282, 88, 170, 993, 154, 831, 'x0_Quenton Narcisse', 705, 357, 551, 578, 935, 'x0_Erica Swallow', 'x0_Bonnie Wertheim', 671, 'x0_Chris Crowell', 'x0_Jeff Petriello', 320, 279, 848, 633]\n"
     ]
    }
   ],
   "source": [
    "print(keep_cols[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "91e22ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 1519) (27643,)\n",
      "(11847, 1519)\n"
     ]
    }
   ],
   "source": [
    "x_train = df_train_concat.drop(['Popularity'], axis=1).to_numpy()\n",
    "x_test = df_test_concat.to_numpy()\n",
    "y_train = df_train['Popularity'].to_numpy()\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "96c45436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.concatenate([\n",
    "# #                             train_topic_tfidf.toarray(), \n",
    "# #                             train_title_tfidf.toarray(), \n",
    "# #                           train_content_tfidf.toarray(),\n",
    "\n",
    "                            \n",
    "# #                         train_content_svd,\n",
    "#                             train_topic_hash.toarray(), \n",
    "# #                             train_title_hash.toarray(),\n",
    "# #                             train_content_hash.toarray(),\n",
    "\n",
    "#                           train_ohe_channel,\n",
    "#                           train_ohe_weekday,\n",
    "#                           train_ohe_author, \n",
    "#                           np.expand_dims(df_train['is_weekend'].values, axis=-1),\n",
    "#                           np.expand_dims(df_train['img count'].values, axis=-1),\n",
    "#                           np.expand_dims(df_train['media count'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_tokens_title'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_tokens_content'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_non_stop_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['num_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['num_self_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['day_of_month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['hour'].values, axis=-1),\n",
    "# #                             sentiment\n",
    "#                             np.expand_dims(df_train['global_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['global_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['title_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['abs_title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['abs_title_sentiment_polarity'].values, axis=-1),\n",
    "# #                                 word sentiment\n",
    "#                             np.expand_dims(df_train['rate_positive_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['rate_negative_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['avg_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['min_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['max_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['avg_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['min_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_train['max_negative_polarity'].values, axis=-1),\n",
    "\n",
    "\n",
    "#                             ], axis=1)\n",
    "\n",
    "# x_test = np.concatenate([\n",
    "# #                             test_topic_tfidf.toarray(), \n",
    "# #                             test_title_tfidf.toarray(), \n",
    "# #                           test_content_tfidf.toarray(),\n",
    "    \n",
    "# #                         test_content_svd,\n",
    "#                             test_topic_hash.toarray(), \n",
    "# #                             test_title_hash.toarray(),\n",
    "# #                             test_content_hash.toarray(),\n",
    "\n",
    "#                           test_ohe_channel,\n",
    "#                           test_ohe_weekday,\n",
    "#                           test_ohe_author, \n",
    "#                           np.expand_dims(df_test['is_weekend'].values, axis=-1),\n",
    "#                           np.expand_dims(df_test['img count'].values, axis=-1),\n",
    "#                           np.expand_dims(df_test['media count'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_tokens_title'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_tokens_content'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_non_stop_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['num_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['num_self_hrefs'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['day_of_month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['month'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['hour'].values, axis=-1),\n",
    "# #                             sentiment\n",
    "#                             np.expand_dims(df_test['global_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['global_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['title_sentiment_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['abs_title_subjectivity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['abs_title_sentiment_polarity'].values, axis=-1),\n",
    "# #                                 word sentiment\n",
    "#                             np.expand_dims(df_test['rate_positive_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['rate_negative_words'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['avg_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['min_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['max_positive_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['avg_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['min_negative_polarity'].values, axis=-1),\n",
    "#                             np.expand_dims(df_test['max_negative_polarity'].values, axis=-1),\n",
    "#                             ], axis=1)\n",
    "\n",
    "# y_train = df_train['Popularity'].to_numpy()\n",
    "# y_train[y_train==-1] = 0\n",
    "\n",
    "# print(x_train.shape, y_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "81d24483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)\n",
    "# kfold = StratifiedKFold(n_splits = 5, random_state = 2021 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "963869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "# d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "2a4da910",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.05, \n",
    "              'max_depth': 4, \n",
    "              'subsample': 0.7 ,\n",
    "              'colsample_bytree': 0.7,\n",
    "              'min_child_weight' : 6,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc', \n",
    "#               'lambda': 1.5,\n",
    "              'alpha': 0.005,\n",
    "#               'n_estimators': 119,\n",
    "             }\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_params = xgb_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "42fb5855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.57940+0.00429\ttest-auc:0.57197+0.00447\n",
      "[10]\ttrain-auc:0.60738+0.00165\ttest-auc:0.58221+0.00759\n",
      "[20]\ttrain-auc:0.61855+0.00196\ttest-auc:0.58343+0.00730\n",
      "[30]\ttrain-auc:0.62741+0.00221\ttest-auc:0.58290+0.00821\n",
      "[40]\ttrain-auc:0.63561+0.00158\ttest-auc:0.58256+0.00840\n",
      "[50]\ttrain-auc:0.64362+0.00137\ttest-auc:0.58229+0.00781\n",
      "[60]\ttrain-auc:0.65024+0.00131\ttest-auc:0.58184+0.00746\n",
      "[69]\ttrain-auc:0.65647+0.00164\ttest-auc:0.58157+0.00790\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, d_train, num_boost_round=1000, verbose_eval=10, nfold=5, metrics=['auc'],\n",
    "     early_stopping_rounds=50, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ae2979e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2472cb6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=None, booster=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7, enable_categorical=False, eta=0.05,\n",
       "              eval_metric='auc', gamma=None, gpu_id=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None,\n",
       "              max_delta_step=None, max_depth=4, min_child_weight=6, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=311, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None,\n",
       "              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n",
       "              subsample=0.7, tree_method=None, validate_parameters=None, ...)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "046237c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/harry/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.005, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7,\n",
       "              enable_categorical=False, eta=0.05, eval_metric='auc', gamma=0,\n",
       "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.0500000007, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=6, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=311, n_jobs=36, num_parallel_tree=1,\n",
       "              predictor='auto', random_state=0, reg_alpha=0.00499999989,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=0.7,\n",
       "              tree_method='exact', validate_parameters=1, ...)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(x_train, y_train, eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "216c8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb_minmax_top1000_feat.csv'\n",
    "y_pred = xgb_model.predict_proba(x_test)[:,1]\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69788e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13753a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc997472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "#               'learning_rate': stats.uniform(0.01, 0.07),\n",
    "#               'subsample': stats.uniform(0.3, 0.7),\n",
    "#               'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "#               'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "#               'min_child_weight': [1, 2, 3]\n",
    "#              }\n",
    "\n",
    "# fit_params = {\n",
    "#     'eval_metric': \"auc\",\n",
    "#     'early_stopping_rounds': 50,\n",
    "#     'verbose' :  True,\n",
    "#     'objective' :'binary:logistic'\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(**fit_params)\n",
    "# clf = RandomizedSearchCV(xgb_model, \n",
    "#                          param_distributions = param_dist, \n",
    "#                          n_iter = 20, \n",
    "#                          scoring='roc_auc', \n",
    "#                          verbose = 3, \n",
    "#                          cv=kfold,\n",
    "#                          n_jobs = 2, refit=True)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_metric': \"auc\",\n",
    "    'early_stopping_rounds': 30,\n",
    "    'verbose' :  True,\n",
    "    'objective' :'binary:logistic'\n",
    "}\n",
    "xgb_model = xgb.XGBClassifier(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "model = xgb.train(xgb_params, d_train, 2000, watchlist, verbose_eval=10, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb.csv'\n",
    "y_pred = model.predict(d_test)\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbab91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88820dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "# get sort indices in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            [indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa568c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94173b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab070daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a706ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3c5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
