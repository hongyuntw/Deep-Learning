{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "449487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bdf59f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('./train_feature.csv')\n",
    "df_test = pd.read_csv('./test_feature.csv')\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6760a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "wnl = WordNetLemmatizer()\n",
    "def prep(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "#     token  = text.split()\n",
    "    \n",
    "#     text = [porter.stem(w) for w in token if w not in stop]\n",
    "    \n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "#     return \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4059b93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'day_of_month', 'month', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "951ea298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the numerical features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['img count', 'media count', 'n_tokens_content', 'n_tokens_title', 'num_hrefs', 'num_self_hrefs']\n",
    "df_train[numerical] = scaler.fit_transform(df_train[numerical])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9a33db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical = ['img count', 'media count']\n",
    "df_test[numerical] = scaler.transform(df_test[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd24544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 19 Jun 2013 15:04:30 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>There may be killer asteroids headed for Eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.069166</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.594783</td>\n",
       "      <td>0.436522</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 28 Mar 2013 17:40:55 +0000</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Google took a stand of sorts against patent-l...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>0.511475</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 07 May 2014 19:15:20 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>You've spend countless hours training to be a...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.135857</td>\n",
       "      <td>0.527828</td>\n",
       "      <td>0.612208</td>\n",
       "      <td>0.451526</td>\n",
       "      <td>0.029032</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Fri, 11 Oct 2013 02:26:50 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>Tired of the same old sports fails and ne...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.031923</td>\n",
       "      <td>0.733577</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.591241</td>\n",
       "      <td>0.035484</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 17 Apr 2014 03:31:43 +0000</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>At 6-foot-5 and 298 pounds, All-Pro NFL star ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.167533</td>\n",
       "      <td>0.516058</td>\n",
       "      <td>0.734307</td>\n",
       "      <td>0.478832</td>\n",
       "      <td>0.045161</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27638</th>\n",
       "      <td>27638</td>\n",
       "      <td>-1</td>\n",
       "      <td>cuba Internet freedom U.S. World USAID</td>\n",
       "      <td>world</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Tue, 08 Apr 2014 16:26:31 +0000</td>\n",
       "      <td>Lorenzo Franceschi-Bicchierai</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>Chief of USAID Doesn't Know Who Created 'Cuban...</td>\n",
       "      <td>The chief of the U.S. Agency for Internationa...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.035511</td>\n",
       "      <td>0.551155</td>\n",
       "      <td>0.551155</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27639</th>\n",
       "      <td>27639</td>\n",
       "      <td>-1</td>\n",
       "      <td>Apps and Software Dev &amp; Design Gadgets Hardwar...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 09 Jul 2014 01:03:24 +0000</td>\n",
       "      <td>Adario Strange</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>Photo of Samsung's Rumored Virtual Reality Hea...</td>\n",
       "      <td>Back in May, reports surfaced claiming that S...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.037243</td>\n",
       "      <td>0.548896</td>\n",
       "      <td>0.589905</td>\n",
       "      <td>0.429022</td>\n",
       "      <td>0.067742</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27640</th>\n",
       "      <td>27640</td>\n",
       "      <td>-1</td>\n",
       "      <td>Food hot dogs humor Photography Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 10 Jul 2014 12:30:13 +0000</td>\n",
       "      <td>Christine Erickson</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>14 Dogs That Frankly Cannot Take the Heat</td>\n",
       "      <td>There's nothing more helpless than the middle...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>0.067742</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27641</th>\n",
       "      <td>27641</td>\n",
       "      <td>-1</td>\n",
       "      <td>Business marissa mayer Media stocks Yahoo</td>\n",
       "      <td>business</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Tue, 16 Apr 2013 20:49:16 +0000</td>\n",
       "      <td>Seth Fiegerman</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>Yahoo Earnings Beat Estimates, But Core Proble...</td>\n",
       "      <td>Yahoo's profits in the first quarter beat Wal...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>0.586047</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.467442</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27642</th>\n",
       "      <td>27642</td>\n",
       "      <td>1</td>\n",
       "      <td>austin Business CurioCity Small Business Startups</td>\n",
       "      <td>small-business</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Fri, 17 Oct 2014 18:22:43 +0000</td>\n",
       "      <td>Megan Ranney</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>The winners of our #CurioCity contest tour Aus...</td>\n",
       "      <td>Originality. Creativity. Ingenuity. In addit...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.056174</td>\n",
       "      <td>0.536170</td>\n",
       "      <td>0.653191</td>\n",
       "      <td>0.455319</td>\n",
       "      <td>0.067742</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27643 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id  Popularity                                              topic  \\\n",
       "0          0          -1  Asteroid Asteroids challenge Earth Space U.S. ...   \n",
       "1          1           1  Apps and Software Google open source opn pledg...   \n",
       "2          2           1      Entertainment NFL NFL Draft Sports Television   \n",
       "3          3          -1                    Sports Video Videos Watercooler   \n",
       "4          4          -1  Entertainment instagram instagram video NFL Sp...   \n",
       "...      ...         ...                                                ...   \n",
       "27638  27638          -1             cuba Internet freedom U.S. World USAID   \n",
       "27639  27639          -1  Apps and Software Dev & Design Gadgets Hardwar...   \n",
       "27640  27640          -1        Food hot dogs humor Photography Watercooler   \n",
       "27641  27641          -1          Business marissa mayer Media stocks Yahoo   \n",
       "27642  27642           1  austin Business CurioCity Small Business Startups   \n",
       "\n",
       "              channel weekday                         pub_date  \\\n",
       "0               world     Wed  Wed, 19 Jun 2013 15:04:30 +0000   \n",
       "1                tech     Thu  Thu, 28 Mar 2013 17:40:55 +0000   \n",
       "2       entertainment     Wed  Wed, 07 May 2014 19:15:20 +0000   \n",
       "3         watercooler     Fri  Fri, 11 Oct 2013 02:26:50 +0000   \n",
       "4       entertainment     Thu  Thu, 17 Apr 2014 03:31:43 +0000   \n",
       "...               ...     ...                              ...   \n",
       "27638           world     Tue  Tue, 08 Apr 2014 16:26:31 +0000   \n",
       "27639            tech     Wed  Wed, 09 Jul 2014 01:03:24 +0000   \n",
       "27640     watercooler     Thu  Thu, 10 Jul 2014 12:30:13 +0000   \n",
       "27641        business     Tue  Tue, 16 Apr 2013 20:49:16 +0000   \n",
       "27642  small-business     Fri  Fri, 17 Oct 2014 18:22:43 +0000   \n",
       "\n",
       "                              author  img count  \\\n",
       "0                                NaN   0.008929   \n",
       "1                   Christina Warren   0.017857   \n",
       "2                          Sam Laird   0.017857   \n",
       "3                          Sam Laird   0.008929   \n",
       "4                    Connor Finnegan   0.464286   \n",
       "...                              ...        ...   \n",
       "27638  Lorenzo Franceschi-Bicchierai   0.017857   \n",
       "27639                 Adario Strange   0.026786   \n",
       "27640             Christine Erickson   0.133929   \n",
       "27641                 Seth Fiegerman   0.026786   \n",
       "27642                   Megan Ranney   0.008929   \n",
       "\n",
       "                                                   title  \\\n",
       "0      NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1      Google's New Open Source Patent Pledge: We Won...   \n",
       "2      Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "3            Cameraperson Fails Deliver Slapstick Laughs   \n",
       "4      NFL Star Helps Young Fan Prove Friendship With...   \n",
       "...                                                  ...   \n",
       "27638  Chief of USAID Doesn't Know Who Created 'Cuban...   \n",
       "27639  Photo of Samsung's Rumored Virtual Reality Hea...   \n",
       "27640          14 Dogs That Frankly Cannot Take the Heat   \n",
       "27641  Yahoo Earnings Beat Estimates, But Core Proble...   \n",
       "27642  The winners of our #CurioCity contest tour Aus...   \n",
       "\n",
       "                                                 content  ...  n_tokens_title  \\\n",
       "0       There may be killer asteroids headed for Eart...  ...        0.333333   \n",
       "1       Google took a stand of sorts against patent-l...  ...        0.555556   \n",
       "2       You've spend countless hours training to be a...  ...        0.555556   \n",
       "3           Tired of the same old sports fails and ne...  ...        0.166667   \n",
       "4       At 6-foot-5 and 298 pounds, All-Pro NFL star ...  ...        0.444444   \n",
       "...                                                  ...  ...             ...   \n",
       "27638   The chief of the U.S. Agency for Internationa...  ...        0.388889   \n",
       "27639   Back in May, reports surfaced claiming that S...  ...        0.388889   \n",
       "27640   There's nothing more helpless than the middle...  ...        0.333333   \n",
       "27641   Yahoo's profits in the first quarter beat Wal...  ...        0.333333   \n",
       "27642    Originality. Creativity. Ingenuity. In addit...  ...        0.444444   \n",
       "\n",
       "       n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0              0.069166         0.530435          0.594783   \n",
       "1              0.035758         0.511475          0.606557   \n",
       "2              0.135857         0.527828          0.612208   \n",
       "3              0.031923         0.733577          0.715328   \n",
       "4              0.167533         0.516058          0.734307   \n",
       "...                 ...              ...               ...   \n",
       "27638          0.035511         0.551155          0.551155   \n",
       "27639          0.037243         0.548896          0.589905   \n",
       "27640          0.019055         0.647059          0.664706   \n",
       "27641          0.051225         0.586047          0.604651   \n",
       "27642          0.056174         0.536170          0.653191   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  day_of_month  \\\n",
       "0                      0.436522   0.064516        0.000000            19   \n",
       "1                      0.377049   0.051613        0.025424            28   \n",
       "2                      0.451526   0.029032        0.033898             7   \n",
       "3                      0.591241   0.035484        0.050847            11   \n",
       "4                      0.478832   0.045161        0.059322            17   \n",
       "...                         ...        ...             ...           ...   \n",
       "27638                  0.415842   0.032258        0.033898             8   \n",
       "27639                  0.429022   0.067742        0.042373             9   \n",
       "27640                  0.464706   0.067742        0.016949            10   \n",
       "27641                  0.467442   0.048387        0.059322            16   \n",
       "27642                  0.455319   0.067742        0.008475            17   \n",
       "\n",
       "       month  hour  \n",
       "0          6    15  \n",
       "1          3    17  \n",
       "2          5    19  \n",
       "3         10     2  \n",
       "4          4     3  \n",
       "...      ...   ...  \n",
       "27638      4    16  \n",
       "27639      7     1  \n",
       "27640      7    12  \n",
       "27641      4    20  \n",
       "27642     10    18  \n",
       "\n",
       "[27643 rows x 21 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2006f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topic = df_train['topic'].values.astype('U').tolist() + df_test['topic'].values.astype('U').tolist()\n",
    "all_channel = df_train['channel'].values.astype('U').tolist() + df_test['channel'].values.astype('U').tolist()\n",
    "all_titles = df_train['title'].values.astype('U').tolist() + df_test['title'].values.astype('U').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b458f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents = list(df_train['content'].values) \n",
    "test_contents = list(df_test['content'].values)\n",
    "all_contents = train_contents + test_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c3658576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  tokenizer=tokenize,\n",
    "#                                   ngram_range=(1,1), \n",
    "#                                   sublinear_tf = True,\n",
    "#                                   max_features = 20000,\n",
    "#                                   dtype = np.float32)\n",
    "# tfidf_vectorize.fit(all_contents)\n",
    "# train_content_tfidf = tfidf_vectorize.transform(train_contents)\n",
    "# test_content_tfidf = tfidf_vectorize.transform(test_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9899c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_content_tfidf.shape)\n",
    "# print(test_content_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f1a5f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = tfidf_vectorize.get_feature_names()\n",
    "# print(vocab[:500])\n",
    "# print(vocab[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ab743180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=1024,\n",
    "                            preprocessor=prep,\n",
    "                            tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4f5157f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  tokenizer=tokenize,\n",
    "                                  ngram_range=(1,1), \n",
    "                                  sublinear_tf = True,\n",
    "                                  max_features = 500,\n",
    "                                  dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "73d128a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/harry/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# topic_tfidf = tfidf_vectorize.fit(all_topic)\n",
    "# train_topic = tfidf_vectorize.transform(df_train['topic'].values.astype('U').tolist())\n",
    "# test_topic = tfidf_vectorize.transform(df_test['topic'].values.astype('U').tolist())\n",
    "\n",
    "print(train_topic.shape)\n",
    "print(test_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2519dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 1024)\n",
      "(11847, 1024)\n"
     ]
    }
   ],
   "source": [
    "train_topic_hash = hashvec.transform(df_train['topic'].values.astype('U').tolist())\n",
    "test_topic_hash = hashvec.transform(df_test['topic'].values.astype('U').tolist())\n",
    "print(train_topic_hash.shape)\n",
    "print(test_topic_hash.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dcd27dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorize = TfidfVectorizer(preprocessor=prep,  tokenizer=tokenize,\n",
    "#                                   ngram_range=(1,1), \n",
    "#                                   sublinear_tf = True,\n",
    "#                                   max_features = 500,\n",
    "#                                   dtype = np.float32)\n",
    "# title_tfidf = tfidf_vectorize.fit(all_titles)\n",
    "# train_title = tfidf_vectorize.transform(df_train['title'].values.astype('U').tolist())\n",
    "# test_title = tfidf_vectorize.transform(df_test['title'].values.astype('U').tolist())\n",
    "\n",
    "# print(train_title.shape)\n",
    "# print(test_title.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e90f3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channle_tfidf = tfidf_vectorize.fit(all_channel)\n",
    "# train_channel = tfidf_vectorize.transform(df_train['channel'].values.astype('U').tolist())\n",
    "# test_channel = tfidf_vectorize.transform(df_test['channel'].values.astype('U').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6803e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_channel.shape)\n",
    "# print(test_channel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "93edc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f740898c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 33)\n",
      "(11847, 33)\n"
     ]
    }
   ],
   "source": [
    "# ohe channel\n",
    "OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "train_ohe_channel = OHE.fit_transform(df_train['channel'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "# OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "test_ohe_channel = OHE.transform(df_test['channel'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_channel.shape)\n",
    "print(test_ohe_channel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "93a465c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 8)\n",
      "(11847, 8)\n"
     ]
    }
   ],
   "source": [
    "# ohe weekday\n",
    "OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "all_week_day = list(df_train['weekday'].values) +  list(df_test['weekday'].values)\n",
    "OHE.fit(np.array(all_week_day).reshape(-1,1))\n",
    "\n",
    "train_ohe_weekday = OHE.transform(df_train['weekday'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_weekday = OHE.transform(df_test['weekday'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_weekday.shape)\n",
    "print(test_ohe_weekday.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "055376c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 470)\n",
      "(11847, 470)\n"
     ]
    }
   ],
   "source": [
    "# ohe author\n",
    "OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "all_author = list(df_train['author'].values) +  list(df_test['author'].values)\n",
    "OHE.fit(np.array(all_author).reshape(-1,1))\n",
    "\n",
    "train_ohe_author = OHE.transform(df_train['author'].values.reshape(-1,1)).toarray()\n",
    "test_ohe_author = OHE.transform(df_test['author'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "print(train_ohe_author.shape)\n",
    "print(test_ohe_author.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "91e22ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Popularity', 'topic', 'channel', 'weekday', 'pub_date', 'author',\n",
       "       'img count', 'title', 'content', 'media count', 'n_tokens_title',\n",
       "       'n_tokens_content', 'n_unique_tokens', 'n_non_stop_words',\n",
       "       'n_non_stop_unique_tokens', 'num_hrefs', 'num_self_hrefs',\n",
       "       'day_of_month', 'month', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "96c45436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 523) (27643,)\n",
      "(11847, 523)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.concatenate([\n",
    "#                           train_content_tfidf.toarray(),\n",
    "#                           train_title.toarray(), \n",
    "#                           train_topic.toarray(), \n",
    "#                             train_topic_hash.toarray(), \n",
    "\n",
    "                          train_ohe_channel,\n",
    "                          train_ohe_weekday,\n",
    "                          train_ohe_author, \n",
    "                          np.expand_dims(df_train['img count'].values, axis=-1),\n",
    "                          np.expand_dims(df_train['media count'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['n_tokens_title'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['n_tokens_content'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['n_unique_tokens'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['n_non_stop_words'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['num_hrefs'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['num_self_hrefs'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['day_of_month'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['month'].values, axis=-1),\n",
    "                            np.expand_dims(df_train['hour'].values, axis=-1),\n",
    "\n",
    "                            ], axis=1)\n",
    "\n",
    "x_test = np.concatenate([\n",
    "#                            test_content_tfidf.toarray(),\n",
    "#                           test_title.toarray(), \n",
    "#                           test_topic.toarray(), \n",
    "#                             test_topic_hash.toarray(), \n",
    "\n",
    "                          test_ohe_channel,\n",
    "                          test_ohe_weekday,\n",
    "                          test_ohe_author, \n",
    "                          np.expand_dims(df_test['img count'].values, axis=-1),\n",
    "                          np.expand_dims(df_test['media count'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['n_tokens_title'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['n_tokens_content'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['n_unique_tokens'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['n_non_stop_words'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['n_non_stop_unique_tokens'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['num_hrefs'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['num_self_hrefs'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['day_of_month'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['month'].values, axis=-1),\n",
    "                            np.expand_dims(df_test['hour'].values, axis=-1),\n",
    "                            ], axis=1)\n",
    "\n",
    "y_train = df_train['Popularity'].to_numpy()\n",
    "y_train[y_train==-1] = 0\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "81d24483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2afbfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "963869a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "# d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c33bb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 5, random_state = 2021 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "af54954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dist = {'n_estimators': stats.randint(150, 500),\n",
    "#               'learning_rate': stats.uniform(0.01, 0.07),\n",
    "#               'subsample': stats.uniform(0.3, 0.7),\n",
    "#               'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "#               'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "#               'min_child_weight': [1, 2, 3]\n",
    "#              }\n",
    "\n",
    "# fit_params = {\n",
    "#     'eval_metric': \"auc\",\n",
    "#     'early_stopping_rounds': 50,\n",
    "#     'verbose' :  True,\n",
    "#     'objective' :'binary:logistic'\n",
    "# }\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "07f1c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomizedSearchCV(xgb_model, \n",
    "#                          param_distributions = param_dist, \n",
    "#                          n_iter = 20, \n",
    "#                          scoring='roc_auc', \n",
    "#                          verbose = 3, \n",
    "#                          cv=kfold,\n",
    "#                          n_jobs = 2, refit=True)\n",
    "# clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2a4da910",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'eta': 0.05, \n",
    "              'max_depth': 5, \n",
    "              'subsample': 0.8, \n",
    "              'colsample_bytree': 0.8,\n",
    "              'min_child_weight' : 1.5,\n",
    "              'objective': 'binary:logistic', \n",
    "              'eval_metric': 'auc', \n",
    "#               'lambda': 1.5,\n",
    "#               'alpha': 0.6,\n",
    "#               'n_estimators': 119,\n",
    "             }\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_params = xgb_model.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "42fb5855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.58665+0.00162\ttest-auc:0.57602+0.00987\n",
      "[10]\ttrain-auc:0.61243+0.00075\ttest-auc:0.58598+0.00814\n",
      "[20]\ttrain-auc:0.62242+0.00120\ttest-auc:0.58737+0.00800\n",
      "[30]\ttrain-auc:0.63114+0.00158\ttest-auc:0.58812+0.00789\n",
      "[40]\ttrain-auc:0.63963+0.00090\ttest-auc:0.58770+0.00726\n",
      "[50]\ttrain-auc:0.64681+0.00169\ttest-auc:0.58820+0.00781\n",
      "[60]\ttrain-auc:0.65456+0.00124\ttest-auc:0.58787+0.00775\n",
      "[70]\ttrain-auc:0.66122+0.00137\ttest-auc:0.58741+0.00736\n",
      "[80]\ttrain-auc:0.66730+0.00105\ttest-auc:0.58714+0.00747\n",
      "[90]\ttrain-auc:0.67201+0.00120\ttest-auc:0.58739+0.00773\n",
      "[99]\ttrain-auc:0.67657+0.00134\ttest-auc:0.58691+0.00762\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, d_train, num_boost_round=1000, verbose_eval=10, nfold=5, metrics=['auc'],\n",
    "     early_stopping_rounds=50, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae2979e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2472cb6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=0.8,\n",
       "              enable_categorical=False, eta=0.05, eval_metric='auc', gamma=None,\n",
       "              gpu_id=None, importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=5,\n",
       "              min_child_weight=1.5, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "046237c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/harry/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8,\n",
       "              enable_categorical=False, eta=0.05, eval_metric='auc', gamma=0,\n",
       "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.0500000007, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1.5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=36, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.8, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(x_train, y_train, eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "216c8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb_topic_hash.csv'\n",
    "y_pred = xgb_model.predict_proba(x_test)[:,1]\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69788e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13753a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc997472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(x_train, y_train)\n",
    "d_valid = xgb.DMatrix(x_val, y_val)\n",
    "d_test = xgb.DMatrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'eval_metric': \"auc\",\n",
    "    'early_stopping_rounds': 30,\n",
    "    'verbose' :  True,\n",
    "    'objective' :'binary:logistic'\n",
    "}\n",
    "xgb_model = xgb.XGBClassifier(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "model = xgb.train(xgb_params, d_train, 2000, watchlist, verbose_eval=10, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './outputs/xgb.csv'\n",
    "y_pred = model.predict(d_test)\n",
    "df_submission = pd.read_csv('./sample_submission.csv')\n",
    "df_submission['Popularity'] = y_pred\n",
    "df_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbab91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88820dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "# get sort indices in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            [indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa568c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94173b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab070daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a706ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3c5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
