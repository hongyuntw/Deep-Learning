{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4917e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e512ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f96689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b892b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(soup):\n",
    "    if soup.time.has_attr('datetime'):\n",
    "        date = soup.time.attrs['datetime']\n",
    "        day = ' '+ date[0:3]\n",
    "    else:\n",
    "        day  = ' noneday'\n",
    "    return day\n",
    "\n",
    "def fetch_pubday(soup):\n",
    "    if soup.time.has_attr('datetime'):\n",
    "        date = soup.time.attrs['datetime']\n",
    "        pub_day = '' + date[:]\n",
    "    else:\n",
    "        pub_day =  ' noneday'\n",
    "    return pub_day\n",
    "\n",
    "def fetch_channel(soup):\n",
    "    \n",
    "    channel = soup.article['data-channel']\n",
    "    return channel\n",
    "\n",
    "def fetch_img_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    find_all_images = soup.find_all('img')\n",
    "\n",
    "    for i in find_all_images:\n",
    "        c = c+1\n",
    "    return c\n",
    "\n",
    "def fetch_topics(soup):\n",
    "\n",
    "    footer = soup.footer\n",
    "    ta = footer.find_all('a')\n",
    "    topic = []\n",
    "\n",
    "    for t in ta:\n",
    "        topic.append(t.get_text())\n",
    "    topic_text = ' '.join(topic)\n",
    "\n",
    "    return topic_text\n",
    "\n",
    "def fetch_authors(soup):\n",
    "\n",
    "    footer = soup.span\n",
    "    if footer != None:\n",
    "        ta = footer.findAll('a')\n",
    "        authors = []\n",
    "        for t in ta:\n",
    "            authors.append(t.get_text())\n",
    "        if len(authors) == 0:\n",
    "            authors_text = 'NaN'\n",
    "        else:\n",
    "            authors_text = ''.join(authors)\n",
    "    else:\n",
    "        authors_text   = 'NaN'\n",
    "\n",
    "    return authors_text\n",
    "\n",
    "def fetch_titles(soup):\n",
    "    \n",
    "    footer = soup.h1\n",
    "    if footer != None:\n",
    "        titles = footer.get_text()\n",
    "    else:\n",
    "        titles = 'NaN'\n",
    "    return titles\n",
    "\n",
    "def fetch_social_media_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    for frame in soup(\"iframe\"):\n",
    "        if frame.get('src').find(\"youtube\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"instagram\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"vine\") != None:\n",
    "            c = c+1\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def fetch_href(soup):\n",
    "    all_a_tags = soup.find_all('a', href=True)\n",
    "    num_href = len(all_a_tags)\n",
    "    num_self_href = 0\n",
    "    for tag in all_a_tags:\n",
    "        href = tag['href']\n",
    "        if 'mashable' in href:\n",
    "            num_self_href += 1\n",
    "    return num_href, num_self_href\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cb53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contents = df_train['Page content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70c3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datas(texts):\n",
    "    days = []\n",
    "    channels = []\n",
    "    img_counts = []\n",
    "    topics = []\n",
    "    authors = []\n",
    "    titles = []\n",
    "    social_media_counts = []\n",
    "    contents = []\n",
    "    num_hrefs = []\n",
    "    num_self_hrefs = []\n",
    "    pub_days = []\n",
    "    \n",
    "    for text in texts:\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        contents.append(soup.find('article').get_text())\n",
    "        topics.append(fetch_topics(soup))\n",
    "        channels.append(fetch_channel(soup))\n",
    "        days.append(fetch_datetime(soup))\n",
    "        authors.append(fetch_authors(soup))\n",
    "        img_counts.append(fetch_img_count(soup))\n",
    "        titles.append(fetch_titles(soup))\n",
    "        social_media_counts.append(fetch_social_media_count(soup))\n",
    "#         input()\n",
    "        \n",
    "        num_href, num_self_href = fetch_href(soup)\n",
    "        num_hrefs.append(num_href)\n",
    "        num_self_hrefs.append(num_self_href)\n",
    "        pub_days.append(fetch_pubday(soup))\n",
    "        \n",
    "\n",
    "    return days, pub_days, channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156dd628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days, pub_days,  channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs = get_all_datas(df_train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f2eb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(days))\n",
    "print(len(pub_days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))\n",
    "print(len(num_hrefs))\n",
    "print(len(num_self_hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74976138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "import string\n",
    "porter = PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def process(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    \n",
    "    \n",
    "    tokens  = nltk.word_tokenize(text)\n",
    "    tokens = [porter.stem(w) for w in tokens]\n",
    "\n",
    "#     token = text.split()\n",
    "#     text = [porter.stem(w) for w in token if w not in stop]\n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d2f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_n_features(titles, contents):\n",
    "    n_tokens_titles = []\n",
    "    n_tokens_contents = []\n",
    "    n_unique_tokens = []\n",
    "    n_non_stop_words = []\n",
    "    n_non_stop_unique_tokens = []\n",
    "\n",
    "\n",
    "    for title, content in zip(titles, contents):\n",
    "\n",
    "        title_tokens = process(title)\n",
    "        n_tokens_titles.append(len(title_tokens))\n",
    "\n",
    "        content_tokens = process(content)\n",
    "        n_tokens_contents.append(len(content_tokens))\n",
    "        \n",
    "        len_content_tokens = len(content_tokens)\n",
    "        if len(content_tokens) == 0:\n",
    "            print(content)\n",
    "            len_content_tokens = 1\n",
    "            \n",
    "        set_content_token = set(content_tokens)\n",
    "        unique_token_rate = len(set_content_token) / len_content_tokens\n",
    "        n_unique_tokens.append(unique_token_rate)\n",
    "\n",
    "        non_stop_words = [w for w in content_tokens if w not in stop]\n",
    "\n",
    "        non_stop_word_rate = len(non_stop_words) / len_content_tokens\n",
    "        n_non_stop_words.append(non_stop_word_rate)\n",
    "\n",
    "        set_non_stop_words = set(non_stop_words)\n",
    "        n_non_stop_unique_tokens_rate = len(set_non_stop_words) / len_content_tokens\n",
    "        n_non_stop_unique_tokens.append(n_non_stop_unique_tokens_rate)\n",
    "        \n",
    "        \n",
    "#         \n",
    "\n",
    "\n",
    "    return n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens\n",
    "    \n",
    "#     print(n_tokens_titles)\n",
    "#     print(n_tokens_contents)\n",
    "#     print(n_unique_tokens)\n",
    "#     print(n_non_stop_words)\n",
    "#     print(n_non_stop_unique_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a14103d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# global_sentiment_polarity: Text sentiment polarity\n",
    "# global_rate_positive_words: Rate of positive words in the content\n",
    "# global_rate_negative_words: Rate of negative words in the content\n",
    "# rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "# rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "# title_subjectivity: Title subjectivity\n",
    "# title_sentiment_polarity: Title polarity\n",
    "# abs_title_subjectivity: Absolute subjectivity level\n",
    "# abs_title_sentiment_polarity: Absolute polarity level\n",
    "\n",
    "def get_sentiment_features(titles, contents):\n",
    "    global_sentiment_polarity = []\n",
    "    global_subjectivity = []\n",
    "    \n",
    "#     global_rate_positive_words = []\n",
    "#     global_rate_negative_words = []\n",
    "    \n",
    "#     rate_positive_words = []\n",
    "#     rate_negative_words = []\n",
    "    \n",
    "    title_subjectivity_list = []\n",
    "    title_sentiment_polarity_list = []\n",
    "    \n",
    "    abs_title_subjectivity = []\n",
    "    abs_title_sentiment_polarity = []\n",
    "    \n",
    "    for title, content in zip(titles, contents):\n",
    "        title_blob = TextBlob(title)\n",
    "        title_polarity = title_blob.sentiment.polarity\n",
    "        title_subjectivity = title_blob.sentiment.subjectivity\n",
    "\n",
    "        \n",
    "        title_sentiment_polarity_list.append(title_polarity)\n",
    "        title_subjectivity_list.append(title_subjectivity)\n",
    "        abs_title_subjectivity.append(abs(title_subjectivity))\n",
    "        abs_title_sentiment_polarity.append(abs(title_polarity))\n",
    "        \n",
    "        \n",
    "        content_blob = TextBlob(content)\n",
    "        content_polarity = content_blob.sentiment.polarity\n",
    "        content_subjectivity = content_blob.sentiment.subjectivity\n",
    "        global_sentiment_polarity.append(content_polarity)\n",
    "        global_subjectivity.append(content_subjectivity)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return global_sentiment_polarity, global_subjectivity, title_subjectivity_list, title_sentiment_polarity_list, \\\n",
    "abs_title_subjectivity, abs_title_sentiment_polarity\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcdfaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_rate_positive_words: Rate of positive words in the content\n",
    "# global_rate_negative_words: Rate of negative words in the content\n",
    "# rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "# rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "# avg_positive_polarity: Avg. polarity of positive words\n",
    "# min_positive_polarity: Min. polarity of positive words\n",
    "# max_positive_polarity: Max. polarity of positive words\n",
    "# avg_negative_polarity: Avg. polarity of negative words\n",
    "# min_negative_polarity: Min. polarity of negative words\n",
    "# max_negative_polarity: Max. polarity of negative words\n",
    "def get_word_sentiment_features(contents):\n",
    "#     global_rate_positive_words = []\n",
    "#     global_rate_negative_words = []\n",
    "    \n",
    "    rate_positive_words = []\n",
    "    rate_negative_words = []\n",
    "    avg_positive_polarity = []\n",
    "    min_positive_polarity = []\n",
    "    max_positive_polarity = []\n",
    "    avg_negative_polarity = []\n",
    "    min_negative_polarity = []\n",
    "    max_negative_polarity = []\n",
    "    \n",
    "    for content in contents:\n",
    "        content_tokens = process(content)\n",
    "        \n",
    "        pos_count = 0\n",
    "        pos_score = 0.0\n",
    "        min_pos_polarity = 1.0\n",
    "        max_pos_polarity = 0.0\n",
    "        \n",
    "        neg_count = 0\n",
    "        neg_score = 0.0\n",
    "        min_neg_polarity = 0.0\n",
    "        max_neg_polarity = -1.0\n",
    "        \n",
    "        for token in content_tokens:\n",
    "            blob = TextBlob(token)\n",
    "            sentiment_score = blob.sentiment.polarity\n",
    "#             positive\n",
    "            if sentiment_score > 0.0:\n",
    "                pos_count += 1\n",
    "                pos_score += sentiment_score\n",
    "                \n",
    "                if sentiment_score < min_pos_polarity:\n",
    "                    min_pos_polarity = sentiment_score\n",
    "                if sentiment_score > max_pos_polarity:\n",
    "                    max_pos_polarity = sentiment_score\n",
    "#             negative\n",
    "            elif sentiment_score < 0.0:\n",
    "                neg_count += 1\n",
    "                neg_score += sentiment_score\n",
    "                \n",
    "                if sentiment_score < min_neg_polarity:\n",
    "                    min_neg_polarity = sentiment_score\n",
    "                if sentiment_score > max_neg_polarity:\n",
    "                    max_neg_polarity = sentiment_score\n",
    "                    \n",
    "        if len(content_tokens) == 0:\n",
    "            rate_positive_words.append(0.0)\n",
    "            rate_negative_words.append(0.0)\n",
    "            avg_positive_polarity.append(0.0)\n",
    "            min_positive_polarity.append(0.0)\n",
    "            max_positive_polarity.append(0.0)\n",
    "            avg_negative_polarity.append(0.0)\n",
    "            min_negative_polarity.append(0.0)\n",
    "            max_negative_polarity.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        if pos_count == 0:\n",
    "            max_pos_polarity = 0.0\n",
    "            min_pos_polarity = 0.0\n",
    "        else:\n",
    "            pos_score /= pos_count\n",
    "            \n",
    "        if neg_count == 0:\n",
    "            min_neg_polarity = 0.0\n",
    "            max_neg_polarity = 0.0\n",
    "        else:\n",
    "            neg_score /= neg_count\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        pos_rate = pos_count / len(content_tokens)\n",
    "        neg_rate = neg_count / len(content_tokens)\n",
    "        rate_positive_words.append(pos_rate)\n",
    "        rate_negative_words.append(neg_rate)\n",
    "        \n",
    "        avg_positive_polarity.append(pos_score)\n",
    "        min_positive_polarity.append(min_pos_polarity)\n",
    "        max_positive_polarity.append(max_pos_polarity)\n",
    "        \n",
    "        avg_negative_polarity.append(neg_score)\n",
    "        min_negative_polarity.append(min_neg_polarity)\n",
    "        max_negative_polarity.append(max_neg_polarity)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity,\\\n",
    "avg_negative_polarity, min_negative_polarity, max_negative_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b83c71ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity,\\\n",
    "avg_negative_polarity, min_negative_polarity, max_negative_polarity = get_word_sentiment_features(contents)\n",
    "\n",
    "print(len(rate_positive_words))\n",
    "print(len(rate_negative_words))\n",
    "print(len(avg_positive_polarity))\n",
    "print(len(min_positive_polarity))\n",
    "print(len(max_positive_polarity))\n",
    "print(len(avg_negative_polarity))\n",
    "print(len(min_negative_polarity))\n",
    "print(len(max_negative_polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e7e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25824af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e58e1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "global_sentiment_polarity, global_subjectivity, title_subjectivity_list, title_sentiment_polarity_list, \\\n",
    "abs_title_subjectivity, abs_title_sentiment_polarity = get_sentiment_features(titles, contents)\n",
    "\n",
    "print(len(global_sentiment_polarity))\n",
    "print(len(global_subjectivity))\n",
    "print(len(title_subjectivity_list))\n",
    "print(len(title_sentiment_polarity_list))\n",
    "print(len(abs_title_subjectivity))\n",
    "print(len(abs_title_sentiment_polarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3270b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "\n",
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "\n",
    "df_train_feature = pd.DataFrame({'Id':df_train.Id[:],\n",
    "                           'Popularity':df_train.Popularity[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                           'pub_date' : pub_days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                            'num_hrefs' : num_hrefs,\n",
    "                            'num_self_hrefs' : num_self_hrefs,\n",
    "                                 'global_sentiment_polarity' : global_sentiment_polarity,\n",
    "                                 'global_subjectivity' : global_subjectivity,\n",
    "                                 'title_subjectivity' : title_subjectivity_list,\n",
    "                                 'title_sentiment_polarity' : title_sentiment_polarity_list,\n",
    "                                 'abs_title_subjectivity' : abs_title_subjectivity,\n",
    "                                 'abs_title_sentiment_polarity' : abs_title_sentiment_polarity,\n",
    "                                 \n",
    "                                 'rate_positive_words' : rate_positive_words,\n",
    "                                'rate_negative_words' : rate_negative_words,\n",
    "                                 'avg_positive_polarity' : avg_positive_polarity,\n",
    "                                 'min_positive_polarity' : min_positive_polarity,\n",
    "                                 'max_positive_polarity' : max_positive_polarity,\n",
    "                                 'avg_negative_polarity' : avg_negative_polarity,\n",
    "                                 'min_negative_polarity' : min_negative_polarity,\n",
    "                                 'max_negative_polarity' : max_negative_polarity,\n",
    "\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7bb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strptime\n",
    "import datetime\n",
    "\n",
    "df_train_feature['day_of_month'] = df_train_feature['pub_date'].apply(lambda x: int(x.split()[1]))\n",
    "df_train_feature['month'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon)\n",
    "# df_train_feature['day_of_week'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[0][:-1], '%a').tm_wday + 1)\n",
    "df_train_feature['hour'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3])\n",
    "# df_train_feature['ymd'] = df_train_feature['pub_date'].apply(lambda x: x[5:7] + '-' + x[8:11] + '-' + x[12:16])\n",
    "#reference: https://www.796t.com/post/aWw5c28=.html\n",
    "# df_train_feature['ymd'] = pd.to_datetime(df_train_feature['ymd'],  format='%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc360e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 19 Jun 2013 15:04:30 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>There may be killer asteroids headed for Eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.465702</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.153571</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 28 Mar 2013 17:40:55 +0000</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Google took a stand of sorts against patent-l...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 07 May 2014 19:15:20 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>You've spend countless hours training to be a...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042002</td>\n",
       "      <td>0.435118</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.433992</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Fri, 11 Oct 2013 02:26:50 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>Tired of the same old sports fails and ne...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040146</td>\n",
       "      <td>0.268214</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.432727</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 17 Apr 2014 03:31:43 +0000</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>At 6-foot-5 and 298 pounds, All-Pro NFL star ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.401529</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.303175</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                              topic  \\\n",
       "0   0          -1  Asteroid Asteroids challenge Earth Space U.S. ...   \n",
       "1   1           1  Apps and Software Google open source opn pledg...   \n",
       "2   2           1      Entertainment NFL NFL Draft Sports Television   \n",
       "3   3          -1                    Sports Video Videos Watercooler   \n",
       "4   4          -1  Entertainment instagram instagram video NFL Sp...   \n",
       "\n",
       "         channel weekday                         pub_date            author  \\\n",
       "0          world     Wed  Wed, 19 Jun 2013 15:04:30 +0000               NaN   \n",
       "1           tech     Thu  Thu, 28 Mar 2013 17:40:55 +0000  Christina Warren   \n",
       "2  entertainment     Wed  Wed, 07 May 2014 19:15:20 +0000         Sam Laird   \n",
       "3    watercooler     Fri  Fri, 11 Oct 2013 02:26:50 +0000         Sam Laird   \n",
       "4  entertainment     Thu  Thu, 17 Apr 2014 03:31:43 +0000   Connor Finnegan   \n",
       "\n",
       "   img count                                              title  \\\n",
       "0          1  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1          2  Google's New Open Source Patent Pledge: We Won...   \n",
       "2          2  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "3          1        Cameraperson Fails Deliver Slapstick Laughs   \n",
       "4         52  NFL Star Helps Young Fan Prove Friendship With...   \n",
       "\n",
       "                                             content  ...  \\\n",
       "0   There may be killer asteroids headed for Eart...  ...   \n",
       "1   Google took a stand of sorts against patent-l...  ...   \n",
       "2   You've spend countless hours training to be a...  ...   \n",
       "3       Tired of the same old sports fails and ne...  ...   \n",
       "4   At 6-foot-5 and 298 pounds, All-Pro NFL star ...  ...   \n",
       "\n",
       "   rate_negative_words  avg_positive_polarity  min_positive_polarity  \\\n",
       "0             0.012174               0.465702               0.136364   \n",
       "1             0.016393               0.386364               0.136364   \n",
       "2             0.042002               0.435118               0.062500   \n",
       "3             0.040146               0.268214               0.100000   \n",
       "4             0.015328               0.401529               0.062500   \n",
       "\n",
       "   max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                    0.8              -0.153571                  -0.25   \n",
       "1                    0.7              -0.130000                  -0.25   \n",
       "2                    1.0              -0.433992                  -1.00   \n",
       "3                    0.5              -0.432727                  -0.60   \n",
       "4                    1.0              -0.303175                  -0.40   \n",
       "\n",
       "   max_negative_polarity  day_of_month  month  hour  \n",
       "0                 -0.125            19      6    15  \n",
       "1                 -0.050            28      3    17  \n",
       "2                 -0.050             7      5    19  \n",
       "3                 -0.150            11     10     2  \n",
       "4                 -0.050            17      4     3  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30fec194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feature.to_csv('./train_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "321fc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "df_test_contents = df_test['Page content'].values.tolist()\n",
    "print(len(df_test_contents))\n",
    "days, pub_days,  channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs = get_all_datas(df_test_contents)\n",
    "print(len(days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))\n",
    "print(len(num_hrefs))\n",
    "print(len(num_self_hrefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f6b21cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    }
   ],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96ca730d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed0814e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "global_sentiment_polarity, global_subjectivity, title_subjectivity_list, title_sentiment_polarity_list, \\\n",
    "abs_title_subjectivity, abs_title_sentiment_polarity = get_sentiment_features(titles, contents)\n",
    "\n",
    "print(len(global_sentiment_polarity))\n",
    "print(len(global_subjectivity))\n",
    "print(len(title_subjectivity_list))\n",
    "print(len(title_sentiment_polarity_list))\n",
    "print(len(abs_title_subjectivity))\n",
    "print(len(abs_title_sentiment_polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380156d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "rate_positive_words, rate_negative_words, avg_positive_polarity, min_positive_polarity, max_positive_polarity,\\\n",
    "avg_negative_polarity, min_negative_polarity, max_negative_polarity = get_word_sentiment_features(contents)\n",
    "\n",
    "print(len(rate_positive_words))\n",
    "print(len(rate_negative_words))\n",
    "print(len(avg_positive_polarity))\n",
    "print(len(min_positive_polarity))\n",
    "print(len(max_positive_polarity))\n",
    "print(len(avg_negative_polarity))\n",
    "print(len(min_negative_polarity))\n",
    "print(len(max_negative_polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49065261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_test_feature = pd.DataFrame({'Id':df_test.Id[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                            'pub_date' : pub_days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                            'num_hrefs' : num_hrefs,\n",
    "                            'num_self_hrefs' : num_self_hrefs,\n",
    "                                'global_sentiment_polarity' : global_sentiment_polarity,\n",
    "                                 'global_subjectivity' : global_subjectivity,\n",
    "                                 'title_subjectivity' : title_subjectivity_list,\n",
    "                                 'title_sentiment_polarity' : title_sentiment_polarity_list,\n",
    "                                 'abs_title_subjectivity' : abs_title_subjectivity,\n",
    "                                 'abs_title_sentiment_polarity' : abs_title_sentiment_polarity,\n",
    "                                \n",
    "                                'rate_positive_words' : rate_positive_words,\n",
    "                                'rate_negative_words' : rate_negative_words,\n",
    "                                 'avg_positive_polarity' : avg_positive_polarity,\n",
    "                                 'min_positive_polarity' : min_positive_polarity,\n",
    "                                 'max_positive_polarity' : max_positive_polarity,\n",
    "                                 'avg_negative_polarity' : avg_negative_polarity,\n",
    "                                 'min_negative_polarity' : min_negative_polarity,\n",
    "                                 'max_negative_polarity' : max_negative_polarity,\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6136429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_feature['day_of_month'] = df_test_feature['pub_date'].apply(lambda x: int(x.split()[1]) if x \n",
    "                                                            != ' noneday' else 0)\n",
    "df_test_feature['month'] = df_test_feature['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon if x \n",
    "                                                             != ' noneday' else 0)\n",
    "df_test_feature['hour'] = df_test_feature['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3] if x  != ' noneday' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c765486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_feature.to_csv('./test_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03865db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda9a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
