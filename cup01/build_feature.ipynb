{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4917e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e512ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f96689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b892b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(soup):\n",
    "    if soup.time.has_attr('datetime'):\n",
    "        date = soup.time.attrs['datetime']\n",
    "        day = ' '+ date[0:3]\n",
    "    else:\n",
    "        day  = ' noneday'\n",
    "    return day\n",
    "\n",
    "def fetch_pubday(soup):\n",
    "    if soup.time.has_attr('datetime'):\n",
    "        date = soup.time.attrs['datetime']\n",
    "        pub_day = '' + date[:]\n",
    "    else:\n",
    "        pub_day =  ' noneday'\n",
    "    return pub_day\n",
    "\n",
    "def fetch_channel(soup):\n",
    "    \n",
    "    channel = soup.article['data-channel']\n",
    "    return channel\n",
    "\n",
    "def fetch_img_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    find_all_images = soup.find_all('img')\n",
    "\n",
    "    for i in find_all_images:\n",
    "        c = c+1\n",
    "    return c\n",
    "\n",
    "def fetch_topics(soup):\n",
    "\n",
    "    footer = soup.footer\n",
    "    ta = footer.find_all('a')\n",
    "    topic = []\n",
    "\n",
    "    for t in ta:\n",
    "        topic.append(t.get_text())\n",
    "    topic_text = ' '.join(topic)\n",
    "\n",
    "    return topic_text\n",
    "\n",
    "def fetch_authors(soup):\n",
    "\n",
    "    footer = soup.span\n",
    "    if footer != None:\n",
    "        ta = footer.findAll('a')\n",
    "        authors = []\n",
    "        for t in ta:\n",
    "            authors.append(t.get_text())\n",
    "        if len(authors) == 0:\n",
    "            authors_text = 'NaN'\n",
    "        else:\n",
    "            authors_text = ''.join(authors)\n",
    "    else:\n",
    "        authors_text   = 'NaN'\n",
    "\n",
    "    return authors_text\n",
    "\n",
    "def fetch_titles(soup):\n",
    "    \n",
    "    footer = soup.h1\n",
    "    if footer != None:\n",
    "        titles = footer.get_text()\n",
    "    else:\n",
    "        titles = 'NaN'\n",
    "    return titles\n",
    "\n",
    "def fetch_social_media_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    for frame in soup(\"iframe\"):\n",
    "        if frame.get('src').find(\"youtube\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"instagram\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"vine\") != None:\n",
    "            c = c+1\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def fetch_href(soup):\n",
    "    all_a_tags = soup.find_all('a', href=True)\n",
    "    num_href = len(all_a_tags)\n",
    "    num_self_href = 0\n",
    "    for tag in all_a_tags:\n",
    "        href = tag['href']\n",
    "        if 'mashable' in href:\n",
    "            num_self_href += 1\n",
    "    return num_href, num_self_href\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cb53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contents = df_train['Page content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70c3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datas(texts):\n",
    "    days = []\n",
    "    channels = []\n",
    "    img_counts = []\n",
    "    topics = []\n",
    "    authors = []\n",
    "    titles = []\n",
    "    social_media_counts = []\n",
    "    contents = []\n",
    "    num_hrefs = []\n",
    "    num_self_hrefs = []\n",
    "    pub_days = []\n",
    "    \n",
    "    for text in texts:\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        contents.append(soup.find('article').get_text())\n",
    "        topics.append(fetch_topics(soup))\n",
    "        channels.append(fetch_channel(soup))\n",
    "        days.append(fetch_datetime(soup))\n",
    "        authors.append(fetch_authors(soup))\n",
    "        img_counts.append(fetch_img_count(soup))\n",
    "        titles.append(fetch_titles(soup))\n",
    "        social_media_counts.append(fetch_social_media_count(soup))\n",
    "#         input()\n",
    "        \n",
    "        num_href, num_self_href = fetch_href(soup)\n",
    "        num_hrefs.append(num_href)\n",
    "        num_self_hrefs.append(num_self_href)\n",
    "        pub_days.append(fetch_pubday(soup))\n",
    "        \n",
    "\n",
    "    return days, pub_days, channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156dd628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days, pub_days,  channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs = get_all_datas(df_train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f2eb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(days))\n",
    "print(len(pub_days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))\n",
    "print(len(num_hrefs))\n",
    "print(len(num_self_hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74976138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def process(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    \n",
    "#     token  = nltk.word_tokenize(text)\n",
    "    token = text.split()\n",
    "#     text = [porter.stem(w) for w in token if w not in stop]\n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d2f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_n_features(titles, contents):\n",
    "    n_tokens_titles = []\n",
    "    n_tokens_contents = []\n",
    "    n_unique_tokens = []\n",
    "    n_non_stop_words = []\n",
    "    n_non_stop_unique_tokens = []\n",
    "\n",
    "\n",
    "    for title, content in zip(titles, contents):\n",
    "\n",
    "        title_tokens = process(title)\n",
    "        n_tokens_titles.append(len(title_tokens))\n",
    "\n",
    "        content_tokens = process(content)\n",
    "        n_tokens_contents.append(len(content_tokens))\n",
    "        \n",
    "        len_content_tokens = len(content_tokens)\n",
    "        if len(content_tokens) == 0:\n",
    "            print(content)\n",
    "            len_content_tokens = 1\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "        set_content_token = set(content_tokens)\n",
    "        unique_token_rate = len(set_content_token) / len_content_tokens\n",
    "        n_unique_tokens.append(unique_token_rate)\n",
    "\n",
    "        non_stop_words = [w for w in content_tokens if w not in stop]\n",
    "\n",
    "        non_stop_word_rate = len(non_stop_words) / len_content_tokens\n",
    "        n_non_stop_words.append(non_stop_word_rate)\n",
    "\n",
    "        set_non_stop_words = set(non_stop_words)\n",
    "        n_non_stop_unique_tokens_rate = len(set_non_stop_words) / len_content_tokens\n",
    "        n_non_stop_unique_tokens.append(n_non_stop_unique_tokens_rate)\n",
    "\n",
    "\n",
    "    return n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens\n",
    "    \n",
    "#     print(n_tokens_titles)\n",
    "#     print(n_tokens_contents)\n",
    "#     print(n_unique_tokens)\n",
    "#     print(n_non_stop_words)\n",
    "#     print(n_non_stop_unique_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e7e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef62992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3270b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "\n",
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "df_train_feature = pd.DataFrame({'Id':df_train.Id[:],\n",
    "                           'Popularity':df_train.Popularity[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                           'pub_date' : pub_days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                            'num_hrefs' : num_hrefs,\n",
    "                            'num_self_hrefs' : num_self_hrefs,\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7bb680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strptime\n",
    "import datetime\n",
    "\n",
    "df_train_feature['day_of_month'] = df_train_feature['pub_date'].apply(lambda x: int(x.split()[1]))\n",
    "df_train_feature['month'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon)\n",
    "# df_train_feature['day_of_week'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[0][:-1], '%a').tm_wday + 1)\n",
    "df_train_feature['hour'] = df_train_feature['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3])\n",
    "# df_train_feature['ymd'] = df_train_feature['pub_date'].apply(lambda x: x[5:7] + '-' + x[8:11] + '-' + x[12:16])\n",
    "#reference: https://www.796t.com/post/aWw5c28=.html\n",
    "# df_train_feature['ymd'] = pd.to_datetime(df_train_feature['ymd'],  format='%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc360e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>...</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 19 Jun 2013 15:04:30 +0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>There may be killer asteroids headed for Eart...</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>575</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.594783</td>\n",
       "      <td>0.436522</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 28 Mar 2013 17:40:55 +0000</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>Google took a stand of sorts against patent-l...</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>305</td>\n",
       "      <td>0.511475</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Wed, 07 May 2014 19:15:20 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>You've spend countless hours training to be a...</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1114</td>\n",
       "      <td>0.527828</td>\n",
       "      <td>0.612208</td>\n",
       "      <td>0.451526</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Fri, 11 Oct 2013 02:26:50 +0000</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>Tired of the same old sports fails and ne...</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>274</td>\n",
       "      <td>0.733577</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.591241</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Thu, 17 Apr 2014 03:31:43 +0000</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>At 6-foot-5 and 298 pounds, All-Pro NFL star ...</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1370</td>\n",
       "      <td>0.516058</td>\n",
       "      <td>0.734307</td>\n",
       "      <td>0.478832</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                              topic  \\\n",
       "0   0          -1  Asteroid Asteroids challenge Earth Space U.S. ...   \n",
       "1   1           1  Apps and Software Google open source opn pledg...   \n",
       "2   2           1      Entertainment NFL NFL Draft Sports Television   \n",
       "3   3          -1                    Sports Video Videos Watercooler   \n",
       "4   4          -1  Entertainment instagram instagram video NFL Sp...   \n",
       "\n",
       "         channel weekday                         pub_date            author  \\\n",
       "0          world     Wed  Wed, 19 Jun 2013 15:04:30 +0000               NaN   \n",
       "1           tech     Thu  Thu, 28 Mar 2013 17:40:55 +0000  Christina Warren   \n",
       "2  entertainment     Wed  Wed, 07 May 2014 19:15:20 +0000         Sam Laird   \n",
       "3    watercooler     Fri  Fri, 11 Oct 2013 02:26:50 +0000         Sam Laird   \n",
       "4  entertainment     Thu  Thu, 17 Apr 2014 03:31:43 +0000   Connor Finnegan   \n",
       "\n",
       "   img count                                              title  \\\n",
       "0          1  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1          2  Google's New Open Source Patent Pledge: We Won...   \n",
       "2          2  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "3          1        Cameraperson Fails Deliver Slapstick Laughs   \n",
       "4         52  NFL Star Helps Young Fan Prove Friendship With...   \n",
       "\n",
       "                                             content  ...  n_tokens_title  \\\n",
       "0   There may be killer asteroids headed for Eart...  ...               8   \n",
       "1   Google took a stand of sorts against patent-l...  ...              12   \n",
       "2   You've spend countless hours training to be a...  ...              12   \n",
       "3       Tired of the same old sports fails and ne...  ...               5   \n",
       "4   At 6-foot-5 and 298 pounds, All-Pro NFL star ...  ...              10   \n",
       "\n",
       "   n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0               575         0.530435          0.594783   \n",
       "1               305         0.511475          0.606557   \n",
       "2              1114         0.527828          0.612208   \n",
       "3               274         0.733577          0.715328   \n",
       "4              1370         0.516058          0.734307   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  day_of_month  month  \\\n",
       "0                  0.436522         22               0            19      6   \n",
       "1                  0.377049         18               3            28      3   \n",
       "2                  0.451526         11               4             7      5   \n",
       "3                  0.591241         13               6            11     10   \n",
       "4                  0.478832         16               7            17      4   \n",
       "\n",
       "   hour  \n",
       "0    15  \n",
       "1    17  \n",
       "2    19  \n",
       "3     2  \n",
       "4     3  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30fec194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feature.to_csv('./train_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "321fc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "df_test_contents = df_test['Page content'].values.tolist()\n",
    "print(len(df_test_contents))\n",
    "days, pub_days,  channels, img_counts, topics, authors, titles, social_media_counts, contents, num_hrefs, num_self_hrefs = get_all_datas(df_test_contents)\n",
    "print(len(days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))\n",
    "print(len(num_hrefs))\n",
    "print(len(num_self_hrefs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f6b21cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    }
   ],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ca730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n",
      "11847\n"
     ]
    }
   ],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49065261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_test_feature = pd.DataFrame({'Id':df_test.Id[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                            'pub_date' : pub_days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens,\n",
    "                            'num_hrefs' : num_hrefs,\n",
    "                            'num_self_hrefs' : num_self_hrefs,\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6136429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_feature['day_of_month'] = df_test_feature['pub_date'].apply(lambda x: int(x.split()[1]) if x \n",
    "                                                            != ' noneday' else 0)\n",
    "df_test_feature['month'] = df_test_feature['pub_date'].apply(lambda x: strptime(x.split()[2], '%b').tm_mon if x \n",
    "                                                             != ' noneday' else 0)\n",
    "df_test_feature['hour'] = df_test_feature['pub_date'].apply(lambda x: strptime(x.split()[4], '%X')[3] if x  != ' noneday' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c765486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_feature.to_csv('./test_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03865db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda9a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
