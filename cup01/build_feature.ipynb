{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4917e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nlplab/harry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e512ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f96689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    stop = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b892b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_datetime(soup):\n",
    "    if soup.time.has_attr('datetime'):\n",
    "        date = soup.time.attrs['datetime']\n",
    "        day = ' '+ date[0:3]\n",
    "    else:\n",
    "        day  = ' fuckday'\n",
    "    return day\n",
    "\n",
    "def fetch_channel(soup):\n",
    "    \n",
    "    channel = soup.article['data-channel']\n",
    "    return channel\n",
    "\n",
    "def fetch_img_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    find_all_images = soup.find_all('img')\n",
    "\n",
    "    for i in find_all_images:\n",
    "        c = c+1\n",
    "    return c\n",
    "\n",
    "def fetch_topics(soup):\n",
    "\n",
    "    footer = soup.footer\n",
    "    ta = footer.find_all('a')\n",
    "    topic = []\n",
    "\n",
    "    for t in ta:\n",
    "        topic.append(t.get_text())\n",
    "    topic_text = ' '.join(topic)\n",
    "\n",
    "    return topic_text\n",
    "\n",
    "def fetch_authors(soup):\n",
    "\n",
    "    footer = soup.span\n",
    "    if footer != None:\n",
    "        ta = footer.findAll('a')\n",
    "        authors = []\n",
    "        for t in ta:\n",
    "            authors.append(t.get_text())\n",
    "        if len(authors) == 0:\n",
    "            authors_text = 'NaN'\n",
    "        else:\n",
    "            authors_text = ''.join(authors)\n",
    "    else:\n",
    "        authors_text   = 'NaN'\n",
    "\n",
    "    return authors_text\n",
    "\n",
    "def fetch_titles(soup):\n",
    "    \n",
    "    footer = soup.h1\n",
    "    if footer != None:\n",
    "        titles = footer.get_text()\n",
    "    else:\n",
    "        titles = 'NaN'\n",
    "    return titles\n",
    "\n",
    "def fetch_social_media_count(soup):\n",
    "\n",
    "    c = 0\n",
    "    for frame in soup(\"iframe\"):\n",
    "        if frame.get('src').find(\"youtube\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"instagram\") != None:\n",
    "            c = c+1\n",
    "        elif frame.get('src').find(\"vine\") != None:\n",
    "            c = c+1\n",
    "\n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64cb53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_contents = df_train['Page content'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70c3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_datas(texts):\n",
    "    days = []\n",
    "    channels = []\n",
    "    img_counts = []\n",
    "    topics = []\n",
    "    authors = []\n",
    "    titles = []\n",
    "    social_media_counts = []\n",
    "    contents = []\n",
    "    \n",
    "    for text in texts:\n",
    "        soup = BeautifulSoup(text, \"lxml\")\n",
    "        contents.append(soup.find('article').get_text())\n",
    "        topics.append(fetch_topics(soup))\n",
    "        channels.append(fetch_channel(soup))\n",
    "        days.append(fetch_datetime(soup))\n",
    "        authors.append(fetch_authors(soup))\n",
    "        img_counts.append(fetch_img_count(soup))\n",
    "        titles.append(fetch_titles(soup))\n",
    "        social_media_counts.append(fetch_social_media_count(soup))\n",
    "#         input()\n",
    "\n",
    "    return days, channels, img_counts, topics, authors, titles, social_media_counts, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156dd628",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days, channels, img_counts, topics, authors, titles, social_media_counts, contents = get_all_datas(df_train_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f2eb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74976138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def process(text):\n",
    "    \n",
    "    # Remove HTML tags.\n",
    "#     text = BeautifulSoup(text,'html.parser').get_text()\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    \n",
    "    token  = nltk.word_tokenize(text)\n",
    "#     text = [porter.stem(w) for w in token if w not in stop]\n",
    "    # Join the words back into one string separated by space, and return the result.\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5d2f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_n_features(titles, contents):\n",
    "    n_tokens_titles = []\n",
    "    n_tokens_contents = []\n",
    "    n_unique_tokens = []\n",
    "    n_non_stop_words = []\n",
    "    n_non_stop_unique_tokens = []\n",
    "\n",
    "\n",
    "    for title, content in zip(titles, contents):\n",
    "\n",
    "        title_tokens = process(title)\n",
    "        n_tokens_titles.append(len(title_tokens))\n",
    "\n",
    "        content_tokens = process(content)\n",
    "        n_tokens_contents.append(len(content_tokens))\n",
    "        \n",
    "        len_content_tokens = len(content_tokens)\n",
    "        if len(content_tokens) == 0:\n",
    "            print(content)\n",
    "            len_content_tokens = 1\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "        set_content_token = set(content_tokens)\n",
    "        unique_token_rate = len(set_content_token) / len_content_tokens\n",
    "        n_unique_tokens.append(unique_token_rate)\n",
    "\n",
    "        non_stop_words = [w for w in content_tokens if w not in stop]\n",
    "\n",
    "        non_stop_word_rate = len(non_stop_words) / len_content_tokens\n",
    "        n_non_stop_words.append(non_stop_word_rate)\n",
    "\n",
    "        set_non_stop_words = set(non_stop_words)\n",
    "        n_non_stop_unique_tokens_rate = len(set_non_stop_words) / len_content_tokens\n",
    "        n_non_stop_unique_tokens.append(n_non_stop_unique_tokens_rate)\n",
    "\n",
    "\n",
    "    return n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens\n",
    "    \n",
    "#     print(n_tokens_titles)\n",
    "#     print(n_tokens_contents)\n",
    "#     print(n_unique_tokens)\n",
    "#     print(n_non_stop_words)\n",
    "#     print(n_non_stop_unique_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e7e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef62992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n",
      "27643\n"
     ]
    }
   ],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3270b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "\n",
    "# 2. n_tokens_title: Number of words in the title\n",
    "# 3. n_tokens_content: Number of words in the content\n",
    "# 4. n_unique_tokens: Rate of unique words in the content\n",
    "# 5. n_non_stop_words: Rate of non-stop words in the content\n",
    "# 6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "df_train_feature = pd.DataFrame({'Id':df_train.Id[:],\n",
    "                           'Popularity':df_train.Popularity[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30fec194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feature.to_csv('./train_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fc771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847\n"
     ]
    }
   ],
   "source": [
    "df_test_contents = df_test['Page content'].values.tolist()\n",
    "print(len(df_test_contents))\n",
    "days, channels, img_counts, topics, authors, titles, social_media_counts, contents = get_all_datas(df_test_contents)\n",
    "print(len(days))\n",
    "print(len(channels))\n",
    "print(len(img_counts))\n",
    "print(len(topics))\n",
    "print(len(authors))\n",
    "print(len(titles))\n",
    "print(len(social_media_counts))\n",
    "print(len(contents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_titles, n_tokens_contents, n_unique_tokens, n_non_stop_words, n_non_stop_unique_tokens = get_some_n_features(titles, contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(n_tokens_titles))\n",
    "print(len(n_tokens_contents))\n",
    "print(len(n_unique_tokens))\n",
    "print(len(n_non_stop_words))\n",
    "print(len(n_non_stop_unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49065261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-consturct training data\n",
    "df_test_feature = pd.DataFrame({'Id':df_test.Id[:],\n",
    "                           'topic':topics,\n",
    "                           'channel':channels,\n",
    "                           'weekday':days,\n",
    "                           'author':authors,\n",
    "                           'img count':img_counts,\n",
    "                           'title':titles,\n",
    "                            'content':contents,\n",
    "                           'media count': social_media_counts,\n",
    "                           'n_tokens_title' : n_tokens_titles,\n",
    "                           'n_tokens_content': n_tokens_contents,\n",
    "                           'n_unique_tokens' : n_unique_tokens,\n",
    "                           'n_non_stop_words': n_non_stop_words,\n",
    "                            'n_non_stop_unique_tokens': n_non_stop_unique_tokens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_feature.to_csv('./test_feature.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945ae5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03865db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda9a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
